{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5fWo_Th88zk",
    "outputId": "9a8b3c6f-be5a-4835-a4fd-63e06b657531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1129 images belonging to 100 classes.\n",
      "Found 322 images belonging to 100 classes.\n",
      "Found 324 images belonging to 100 classes.\n"
     ]
    }
   ],
   "source": [
    "import os # miscellaneous operating system interfaces\n",
    "import shutil # high-level file operations\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import random\n",
    "from itertools import product\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Conv2D \n",
    "from tensorflow.keras.layers import MaxPooling2D \n",
    "from tensorflow.keras.layers import Flatten \n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.applications import ResNet50,MobileNetV2, Xception, DenseNet121,ResNet50V2,ResNet101V2, ResNet152V2,NASNetLarge,InceptionV3,InceptionResNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.layers import  Input, Conv2D, Conv2DTranspose, ReLU,AveragePooling2D, MaxPooling2D, Dense, BatchNormalization, GlobalAveragePooling2D, Concatenate\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/shoes_classifier\"\n",
    "\n",
    "train_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/shoes_classifier/train\"\n",
    "\n",
    "test_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/shoes_classifier/test\"\n",
    "\n",
    "val_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/shoes_classifier/val\"\n",
    "\n",
    "\n",
    "learning_rate = 0.00001\n",
    "train_batch_size = 8 # train data 숫자\n",
    "test_batch_size = 8 # test data 숫자\n",
    "val_batch_size = 8 # val data 숫자\n",
    "\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                 rotation_range = 30,\n",
    "                                 width_shift_range=0.1,\n",
    "                                 height_shift_range=0.1,\n",
    "                                 shear_range=0.05,\n",
    "                                 zoom_range=0.05,\n",
    "                                 horizontal_flip=True,\n",
    "                                 fill_mode='nearest')\n",
    "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "categories = ['nike_coat_legacy'              \n",
    "              ,'adidas_equipment10'\n",
    "              ,'converse_chuckTailor_allstar_classic'\n",
    "              ,'fila_coatDelux_belcro'\n",
    "              ,'puma_kaia_platform'\n",
    "              ,'discovery_jogger_flex'\n",
    "              ,'reebok_royal_bridge3'\n",
    "              ,'newbal_w480'\n",
    "              ,'adidas_superstar'\n",
    "              ,'nike_revolution5'\n",
    "              ,'converse_chuckTailor_dainti_mule'\n",
    "              ,'fila_classic_border_stitch'\n",
    "              ,'puma_smash_bulk_mule'\n",
    "              ,'discovery_bucket_dewalker_v2'\n",
    "              ,'reebok_zig_dynamica'\n",
    "              ,'newbal_ms237'\n",
    "              ,'nike_air_tailwind79'\n",
    "              ,'adidas_responseSR'\n",
    "              ,'fila_como_mule'\n",
    "              ,'puma_thunder_passion2.0'\n",
    "              ,'converse_chuckTailor_move_high'\n",
    "              ,'discovery_bucket_mountain_LT'\n",
    "              ,'reebok_energen_run'\n",
    "              ,'newbal_mr530'\n",
    "              ,'nike_airmax_infinity2'\n",
    "              ,'adidas_brabada'\n",
    "              ,'fila_classic_kicksB'\n",
    "              ,'puma_future_rider_play_on'\n",
    "              ,'converse_chuckTailor_allstar_high'\n",
    "              ,'discovery_brick'\n",
    "              ,'reebok_royal_run_finish'\n",
    "              ,'newbal_sd5205'\n",
    "              ,'nike_coat_legacy_mule'\n",
    "              ,'adidas_retro_bulk'\n",
    "              ,'fila_bumper_mule'\n",
    "              ,'puma_bari_mule'\n",
    "              ,'prospecs_bigstar101'\n",
    "              ,'newbal_mt410'\n",
    "              ,'fila_aztrack96'\n",
    "              ,'converse_chuckTailor_allStar'\n",
    "              ,'puma_interplex_runner'\n",
    "              ,'fila_bumper'\n",
    "              ,'adidas_grand_coat_base'\n",
    "              ,'nike_airmax_vg-r'\n",
    "              ,'prospecs_maharun402'\n",
    "              ,'newbal_ml860'\n",
    "              ,'reebok_royal_run_finish2'\n",
    "              ,'asics_jog_100s'\n",
    "              ,'nike_airmax_bolt'\n",
    "              ,'adidas_alpha_bounce_ek'\n",
    "              ,'fila_rgb_flow'\n",
    "              ,'puma_bari_z'\n",
    "              ,'asics_jel1090'\n",
    "              ,'prospecs_crossboa'\n",
    "              ,'newbal_ml827'\n",
    "              ,'reebok_liquifect_90ap'\n",
    "              ,'nike_wear_allday'\n",
    "              ,'adidas_flood_flow'\n",
    "              ,'fila_classic_kicks'\n",
    "              ,'puma_extra'\n",
    "              ,'reebok_driftium'\n",
    "              ,'newbal_ws109'\n",
    "              ,'prospecs_river_ex511'\n",
    "              ,'asics_jel-contend7'\n",
    "              ,'nike_air_monarch4'\n",
    "              ,'adidas_duramo_sl'\n",
    "              ,'fila_fluid'\n",
    "              ,'puma_cel_endura_patent98'\n",
    "              ,'reebok_royal_complete3_low'\n",
    "              ,'newbal_mflsht'\n",
    "              ,'prospecs_river_ex512'\n",
    "              ,'asics_trabuco_max'\n",
    "              ,'nike_airmax_axi'\n",
    "              ,'adidas_nizza_trefoli'\n",
    "              ,'fila_decypher'\n",
    "              ,'puma_tx-3'\n",
    "              ,'reebok_ridgerider5.0_uni'\n",
    "              ,'newbal_m068'\n",
    "              ,'prospecs_stax_tr'\n",
    "              ,'nike_run_swift2'\n",
    "              ,'adidas_galaxy5'\n",
    "              ,'fila_festibo91'\n",
    "              ,'puma_scotch_runner'\n",
    "              ,'prospecs_cross_tr'\n",
    "              ,'newbal_wx452'\n",
    "              ,'reebok_furylight3'\n",
    "              ,'nike_airmax_sc'\n",
    "              ,'adidas_run_palcon2.0'\n",
    "              ,'fila_funky_tennis1998'\n",
    "              ,'puma_millenio'\n",
    "              ,'prospecs_daytrip'\n",
    "              ,'newbal_yacstr'\n",
    "              ,'reebok_royal_turbo_impulse'\n",
    "              ,'puma_r78_og'\n",
    "              ,'fila_skipper'\n",
    "              ,'fila_ray_tracer'\n",
    "              ,'adidas_hiwi'\n",
    "              ,'adidas_clima_warm2.0u'\n",
    "              ,'nike_coat_vision_alta_txt'\n",
    "              ,'nike_down_shifter11'\n",
    "              ]\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(train_img_dir, target_size=(300,300), \n",
    "                                             classes=categories, \n",
    "                                             batch_size=train_batch_size,\n",
    "                                             class_mode='categorical'\n",
    "                                             )\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(test_img_dir,\n",
    "                                        target_size=(300,300), \n",
    "                                        classes=categories, \n",
    "                                        batch_size=test_batch_size,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "val_set = test_datagen.flow_from_directory(val_img_dir,\n",
    "                                        target_size=(300,300), \n",
    "                                        classes=categories, \n",
    "                                        batch_size=val_batch_size,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "\n",
    "\n",
    "x_train, y_train = next(training_set)\n",
    "x_test, y_test = next(test_set)\n",
    "x_val,y_val = next(val_set)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "5c0O0QEE8-3S"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs = Input(shape=(300, 300, 3))    \n",
    "    application  = ResNet50(weights = \"imagenet\", include_top=False,input_shape = (300,300,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    \n",
    "\n",
    "    output = application.output\n",
    "    # pooling = AveragePooling2D(pool_size=(16,16),padding='SAME')(output)   \n",
    "\n",
    "    flatten1 = Flatten()(output)\n",
    "\n",
    "    # dense1 = Dense(units = 256)(flatten1)\n",
    "    # batch1 = BatchNormalization()(dense1)\n",
    "    # relu1 = ReLU()(batch1)\n",
    "    # dense2 = Dense(units = 64)(relu1)\n",
    "    # batch2 = BatchNormalization()(dense2)\n",
    "    # relu2 = ReLU()(batch2)\n",
    "\n",
    "    dense3 = Dense(units = 100, activation = 'softmax')(flatten1)    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=dense3)\n",
    "    \n",
    "def mobile_net():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    mobileNet = MobileNetV2(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in mobileNet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = mobileNet.output\n",
    "    pooling = AveragePooling2D(pool_size=(16,16),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 256)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 64)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "\n",
    "    dense3 = Dense(units = 100, activation = 'softmax')(relu2)    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def xception():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    xception = Xception(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in xception.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = xception.output\n",
    "    pooling = AveragePooling2D(pool_size=(8,8),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 256)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 64)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "\n",
    "    dense3 = Dense(units = 100, activation = 'softmax')(relu2)    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "\n",
    "def resnet():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    resnet = ResNet152V2 (weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in resnet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = resnet.output\n",
    "    pooling = MaxPooling2D(pool_size=(8,8),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 256)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 64)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "\n",
    "    dense3 = Dense(units = 100, activation = 'softmax')(relu2)    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def densenet():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    densenet = DenseNet121(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in densenet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = densenet.output\n",
    "    pooling = MaxPooling2D(pool_size=(32,32),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 256)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 64)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "\n",
    "    dense3 = Dense(units = 100, activation = 'softmax')(relu2)    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "\n",
    "def inception():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    inception = InceptionV3(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in inception.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = inception.output\n",
    "    pooling = MaxPooling2D(pool_size=(16,16),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 256)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 64)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "\n",
    "    dense3 = Dense(units = 100, activation = 'softmax')(relu2)    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def inceptionresnet():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    inceptionresnet = InceptionResNetV2(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in inceptionresnet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = inceptionresnet.output\n",
    "    pooling = MaxPooling2D(pool_size=(32,32),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 256)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 64)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "\n",
    "    dense3 = Dense(units = 100, activation = 'softmax')(relu2)    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResidualUnit(tf.keras.Model):\n",
    "    def __init__(self, filter_in, filter_out, kernel_size):\n",
    "        super(ResidualUnit, self).__init__()\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same')\n",
    "        \n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same')\n",
    "        \n",
    "        if filter_in == filter_out:\n",
    "            self.identity = lambda x: x\n",
    "        else:\n",
    "            self.identity = tf.keras.layers.Conv2D(filter_out, (1,1), padding='same')\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        h = self.bn1(x, training=training)\n",
    "        h = tf.nn.relu(h)\n",
    "        h = self.conv1(h)\n",
    "        \n",
    "        h = self.bn2(h, training=training)\n",
    "        h = tf.nn.relu(h)\n",
    "        h = self.conv2(h)\n",
    "        return self.identity(x) + h\n",
    "    \n",
    "class ResnetLayer(tf.keras.Model):\n",
    "    def __init__(self, filter_in, filters, kernel_size):\n",
    "        super(ResnetLayer, self).__init__()\n",
    "        self.sequence = list()\n",
    "        for f_in, f_out in zip([filter_in] + list(filters), filters):\n",
    "            self.sequence.append(ResidualUnit(f_in, f_out, kernel_size))\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        for unit in self.sequence:\n",
    "            x = unit(x, training=training)\n",
    "        return x\n",
    "    \n",
    "class ResNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu') # 28x28x8\n",
    "        \n",
    "        self.res1 = ResnetLayer(64, (16, 16), (3, 3)) # 28x28x16\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D((2, 2)) # 14x14x16\n",
    "        \n",
    "        \n",
    "        self.res2 = ResnetLayer(128, (32, 32), (3, 3)) # 14x14x32\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
    "    \n",
    "        \n",
    "        self.res3 = ResnetLayer(256, (64, 64), (3, 3)) # 7x7x64\n",
    "        self.pool = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
    "        \n",
    "        \n",
    "        self.res4 = ResnetLayer(512, (64, 64), (3, 3)) # 7x7x64\n",
    "        self.pool = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
    "        \n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(5, activation='softmax')\n",
    "        \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.res1(x, training=training)\n",
    "        x = self.pool1(x)\n",
    "        x = self.res2(x, training=training)\n",
    "        x = self.pool2(x)\n",
    "        x = self.res3(x, training=training)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gy5K2_XL9Ajq",
    "outputId": "292c63ff-9217-4606-8784-ee70df95d82c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "142/142 [==============================] - 43s 271ms/step - loss: 5.9393 - accuracy: 0.0434 - precision: 0.0634 - recall: 0.0141 - f1score: 0.0222 - val_loss: 7.0981 - val_accuracy: 0.0309 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
      "Epoch 2/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 3.2485 - accuracy: 0.2737 - precision: 0.4546 - recall: 0.1532 - f1score: 0.2206 - val_loss: 7.1517 - val_accuracy: 0.0185 - val_precision: 0.0203 - val_recall: 0.0061 - val_f1score: 0.0093\n",
      "Epoch 3/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 2.0871 - accuracy: 0.4765 - precision: 0.6758 - recall: 0.3239 - f1score: 0.4258 - val_loss: 7.2939 - val_accuracy: 0.0278 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
      "Epoch 4/80\n",
      "142/142 [==============================] - 38s 265ms/step - loss: 1.4053 - accuracy: 0.6209 - precision: 0.7750 - recall: 0.5106 - f1score: 0.6074 - val_loss: 6.3553 - val_accuracy: 0.0648 - val_precision: 0.0598 - val_recall: 0.0244 - val_f1score: 0.0336\n",
      "Epoch 5/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.9577 - accuracy: 0.7378 - precision: 0.8413 - recall: 0.6523 - f1score: 0.7284 - val_loss: 4.3890 - val_accuracy: 0.2006 - val_precision: 0.3020 - val_recall: 0.1341 - val_f1score: 0.1782\n",
      "Epoch 6/80\n",
      "142/142 [==============================] - 38s 265ms/step - loss: 0.7218 - accuracy: 0.7963 - precision: 0.8737 - recall: 0.7174 - f1score: 0.7830 - val_loss: 2.7886 - val_accuracy: 0.3951 - val_precision: 0.5195 - val_recall: 0.3110 - val_f1score: 0.3825\n",
      "Epoch 7/80\n",
      "142/142 [==============================] - 38s 268ms/step - loss: 0.5758 - accuracy: 0.8264 - precision: 0.8959 - recall: 0.7773 - f1score: 0.8278 - val_loss: 1.8057 - val_accuracy: 0.5926 - val_precision: 0.6701 - val_recall: 0.5305 - val_f1score: 0.5884\n",
      "Epoch 8/80\n",
      "142/142 [==============================] - 39s 272ms/step - loss: 0.5013 - accuracy: 0.8547 - precision: 0.9006 - recall: 0.8028 - f1score: 0.8449 - val_loss: 1.2874 - val_accuracy: 0.7346 - val_precision: 0.8090 - val_recall: 0.6951 - val_f1score: 0.7441\n",
      "Epoch 9/80\n",
      "142/142 [==============================] - 38s 267ms/step - loss: 0.3784 - accuracy: 0.8849 - precision: 0.9149 - recall: 0.8504 - f1score: 0.8786 - val_loss: 1.1403 - val_accuracy: 0.7716 - val_precision: 0.8331 - val_recall: 0.7530 - val_f1score: 0.7873\n",
      "Epoch 10/80\n",
      "142/142 [==============================] - 38s 269ms/step - loss: 0.3783 - accuracy: 0.8813 - precision: 0.9126 - recall: 0.8486 - f1score: 0.8770 - val_loss: 1.0335 - val_accuracy: 0.7747 - val_precision: 0.8142 - val_recall: 0.7591 - val_f1score: 0.7839\n",
      "Epoch 11/80\n",
      "142/142 [==============================] - 38s 265ms/step - loss: 0.3243 - accuracy: 0.9043 - precision: 0.9325 - recall: 0.8776 - f1score: 0.9020 - val_loss: 0.9379 - val_accuracy: 0.8025 - val_precision: 0.8614 - val_recall: 0.7835 - val_f1score: 0.8185\n",
      "Epoch 12/80\n",
      "142/142 [==============================] - 38s 265ms/step - loss: 0.2742 - accuracy: 0.9283 - precision: 0.9461 - recall: 0.9102 - f1score: 0.9262 - val_loss: 0.9552 - val_accuracy: 0.8025 - val_precision: 0.8368 - val_recall: 0.7927 - val_f1score: 0.8127\n",
      "Epoch 13/80\n",
      "142/142 [==============================] - 39s 271ms/step - loss: 0.2901 - accuracy: 0.9132 - precision: 0.9289 - recall: 0.8873 - f1score: 0.9065 - val_loss: 0.9232 - val_accuracy: 0.8025 - val_precision: 0.8271 - val_recall: 0.7927 - val_f1score: 0.8084\n",
      "Epoch 14/80\n",
      "142/142 [==============================] - 38s 265ms/step - loss: 0.1987 - accuracy: 0.9531 - precision: 0.9598 - recall: 0.9296 - f1score: 0.9433 - val_loss: 0.9402 - val_accuracy: 0.8179 - val_precision: 0.8654 - val_recall: 0.8110 - val_f1score: 0.8352\n",
      "Epoch 15/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.1801 - accuracy: 0.9469 - precision: 0.9659 - recall: 0.9357 - f1score: 0.9497 - val_loss: 1.0166 - val_accuracy: 0.8179 - val_precision: 0.8471 - val_recall: 0.8110 - val_f1score: 0.8275\n",
      "Epoch 16/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.1581 - accuracy: 0.9495 - precision: 0.9564 - recall: 0.9340 - f1score: 0.9444 - val_loss: 0.9855 - val_accuracy: 0.8148 - val_precision: 0.8465 - val_recall: 0.8079 - val_f1score: 0.8254\n",
      "Epoch 17/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.1648 - accuracy: 0.9566 - precision: 0.9559 - recall: 0.9393 - f1score: 0.9470 - val_loss: 0.9007 - val_accuracy: 0.8210 - val_precision: 0.8590 - val_recall: 0.8140 - val_f1score: 0.8344\n",
      "Epoch 18/80\n",
      "142/142 [==============================] - 37s 263ms/step - loss: 0.1915 - accuracy: 0.9477 - precision: 0.9588 - recall: 0.9384 - f1score: 0.9478 - val_loss: 0.9527 - val_accuracy: 0.8148 - val_precision: 0.8479 - val_recall: 0.7988 - val_f1score: 0.8209\n",
      "Epoch 19/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.1809 - accuracy: 0.9460 - precision: 0.9588 - recall: 0.9357 - f1score: 0.9464 - val_loss: 0.9633 - val_accuracy: 0.8148 - val_precision: 0.8570 - val_recall: 0.8049 - val_f1score: 0.8288\n",
      "Epoch 20/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.1457 - accuracy: 0.9575 - precision: 0.9599 - recall: 0.9445 - f1score: 0.9517 - val_loss: 0.8164 - val_accuracy: 0.8272 - val_precision: 0.8667 - val_recall: 0.8201 - val_f1score: 0.8403\n",
      "Epoch 21/80\n",
      "142/142 [==============================] - 37s 263ms/step - loss: 0.0847 - accuracy: 0.9779 - precision: 0.9842 - recall: 0.9718 - f1score: 0.9776 - val_loss: 0.8808 - val_accuracy: 0.8179 - val_precision: 0.8555 - val_recall: 0.8049 - val_f1score: 0.8275\n",
      "Epoch 22/80\n",
      "142/142 [==============================] - 38s 265ms/step - loss: 0.1467 - accuracy: 0.9593 - precision: 0.9644 - recall: 0.9551 - f1score: 0.9594 - val_loss: 0.8257 - val_accuracy: 0.8272 - val_precision: 0.8515 - val_recall: 0.8140 - val_f1score: 0.8305\n",
      "Epoch 23/80\n",
      "142/142 [==============================] - 37s 263ms/step - loss: 0.1188 - accuracy: 0.9699 - precision: 0.9751 - recall: 0.9665 - f1score: 0.9705 - val_loss: 0.8407 - val_accuracy: 0.8488 - val_precision: 0.8743 - val_recall: 0.8293 - val_f1score: 0.8499\n",
      "Epoch 24/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.1099 - accuracy: 0.9743 - precision: 0.9762 - recall: 0.9674 - f1score: 0.9715 - val_loss: 0.8955 - val_accuracy: 0.8364 - val_precision: 0.8600 - val_recall: 0.8140 - val_f1score: 0.8348\n",
      "Epoch 25/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.1335 - accuracy: 0.9637 - precision: 0.9627 - recall: 0.9542 - f1score: 0.9581 - val_loss: 0.9013 - val_accuracy: 0.8302 - val_precision: 0.8653 - val_recall: 0.8232 - val_f1score: 0.8424\n",
      "Epoch 26/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.1281 - accuracy: 0.9708 - precision: 0.9682 - recall: 0.9639 - f1score: 0.9658 - val_loss: 0.8927 - val_accuracy: 0.8272 - val_precision: 0.8571 - val_recall: 0.8110 - val_f1score: 0.8315\n",
      "Epoch 27/80\n",
      "142/142 [==============================] - 37s 263ms/step - loss: 0.1348 - accuracy: 0.9690 - precision: 0.9692 - recall: 0.9604 - f1score: 0.9645 - val_loss: 0.8842 - val_accuracy: 0.8210 - val_precision: 0.8336 - val_recall: 0.8049 - val_f1score: 0.8179\n",
      "Epoch 28/80\n",
      "142/142 [==============================] - 38s 265ms/step - loss: 0.1304 - accuracy: 0.9610 - precision: 0.9635 - recall: 0.9577 - f1score: 0.9604 - val_loss: 0.8690 - val_accuracy: 0.8333 - val_precision: 0.8602 - val_recall: 0.8201 - val_f1score: 0.8388\n",
      "Epoch 29/80\n",
      "142/142 [==============================] - 38s 268ms/step - loss: 0.1188 - accuracy: 0.9734 - precision: 0.9771 - recall: 0.9710 - f1score: 0.9738 - val_loss: 0.8156 - val_accuracy: 0.8395 - val_precision: 0.8731 - val_recall: 0.8354 - val_f1score: 0.8526\n",
      "Epoch 30/80\n",
      "142/142 [==============================] - 38s 269ms/step - loss: 0.0964 - accuracy: 0.9752 - precision: 0.9735 - recall: 0.9683 - f1score: 0.9707 - val_loss: 0.8920 - val_accuracy: 0.8395 - val_precision: 0.8554 - val_recall: 0.8201 - val_f1score: 0.8364\n",
      "Epoch 31/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.0797 - accuracy: 0.9823 - precision: 0.9850 - recall: 0.9815 - f1score: 0.9832 - val_loss: 0.8921 - val_accuracy: 0.8333 - val_precision: 0.8515 - val_recall: 0.8323 - val_f1score: 0.8413\n",
      "Epoch 32/80\n",
      "142/142 [==============================] - 38s 265ms/step - loss: 0.0919 - accuracy: 0.9770 - precision: 0.9733 - recall: 0.9648 - f1score: 0.9687 - val_loss: 0.8737 - val_accuracy: 0.8333 - val_precision: 0.8551 - val_recall: 0.8262 - val_f1score: 0.8393\n",
      "Epoch 33/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.0901 - accuracy: 0.9770 - precision: 0.9726 - recall: 0.9683 - f1score: 0.9702 - val_loss: 0.8523 - val_accuracy: 0.8457 - val_precision: 0.8660 - val_recall: 0.8415 - val_f1score: 0.8528\n",
      "Epoch 34/80\n",
      "142/142 [==============================] - 37s 263ms/step - loss: 0.0929 - accuracy: 0.9761 - precision: 0.9735 - recall: 0.9683 - f1score: 0.9707 - val_loss: 1.0923 - val_accuracy: 0.8395 - val_precision: 0.8592 - val_recall: 0.8293 - val_f1score: 0.8428\n",
      "Epoch 35/80\n",
      "142/142 [==============================] - 37s 263ms/step - loss: 0.1254 - accuracy: 0.9681 - precision: 0.9726 - recall: 0.9665 - f1score: 0.9694 - val_loss: 0.9657 - val_accuracy: 0.8426 - val_precision: 0.8672 - val_recall: 0.8384 - val_f1score: 0.8518\n",
      "Epoch 36/80\n",
      "142/142 [==============================] - 37s 263ms/step - loss: 0.1018 - accuracy: 0.9690 - precision: 0.9688 - recall: 0.9595 - f1score: 0.9638 - val_loss: 0.8676 - val_accuracy: 0.8395 - val_precision: 0.8492 - val_recall: 0.8354 - val_f1score: 0.8416\n",
      "Epoch 37/80\n",
      "142/142 [==============================] - 37s 263ms/step - loss: 0.0637 - accuracy: 0.9849 - precision: 0.9798 - recall: 0.9762 - f1score: 0.9779 - val_loss: 1.0163 - val_accuracy: 0.8426 - val_precision: 0.8689 - val_recall: 0.8354 - val_f1score: 0.8507\n",
      "Epoch 38/80\n",
      "142/142 [==============================] - 37s 263ms/step - loss: 0.0828 - accuracy: 0.9743 - precision: 0.9727 - recall: 0.9648 - f1score: 0.9684 - val_loss: 1.0554 - val_accuracy: 0.8426 - val_precision: 0.8637 - val_recall: 0.8415 - val_f1score: 0.8516\n",
      "Epoch 39/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.0981 - accuracy: 0.9752 - precision: 0.9736 - recall: 0.9665 - f1score: 0.9698 - val_loss: 0.9617 - val_accuracy: 0.8426 - val_precision: 0.8595 - val_recall: 0.8232 - val_f1score: 0.8393\n",
      "Epoch 40/80\n",
      "142/142 [==============================] - 37s 263ms/step - loss: 0.0812 - accuracy: 0.9805 - precision: 0.9795 - recall: 0.9736 - f1score: 0.9763 - val_loss: 0.8685 - val_accuracy: 0.8642 - val_precision: 0.8921 - val_recall: 0.8628 - val_f1score: 0.8763\n",
      "Epoch 41/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.0643 - accuracy: 0.9841 - precision: 0.9789 - recall: 0.9762 - f1score: 0.9775 - val_loss: 1.0106 - val_accuracy: 0.8333 - val_precision: 0.8415 - val_recall: 0.8293 - val_f1score: 0.8347\n",
      "Epoch 42/80\n",
      "142/142 [==============================] - 38s 263ms/step - loss: 0.0876 - accuracy: 0.9690 - precision: 0.9672 - recall: 0.9621 - f1score: 0.9645 - val_loss: 0.9744 - val_accuracy: 0.8580 - val_precision: 0.9005 - val_recall: 0.8537 - val_f1score: 0.8745\n",
      "Epoch 43/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.0883 - accuracy: 0.9787 - precision: 0.9727 - recall: 0.9710 - f1score: 0.9718 - val_loss: 0.9376 - val_accuracy: 0.8488 - val_precision: 0.8589 - val_recall: 0.8384 - val_f1score: 0.8480\n",
      "Epoch 44/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.1060 - accuracy: 0.9734 - precision: 0.9691 - recall: 0.9648 - f1score: 0.9668 - val_loss: 0.9670 - val_accuracy: 0.8364 - val_precision: 0.8563 - val_recall: 0.8384 - val_f1score: 0.8467\n",
      "Epoch 45/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.0712 - accuracy: 0.9805 - precision: 0.9838 - recall: 0.9780 - f1score: 0.9807 - val_loss: 0.8918 - val_accuracy: 0.8488 - val_precision: 0.8705 - val_recall: 0.8445 - val_f1score: 0.8562\n",
      "Epoch 46/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.1023 - accuracy: 0.9752 - precision: 0.9718 - recall: 0.9657 - f1score: 0.9685 - val_loss: 1.1281 - val_accuracy: 0.8302 - val_precision: 0.8611 - val_recall: 0.8293 - val_f1score: 0.8441\n",
      "Epoch 47/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.0737 - accuracy: 0.9832 - precision: 0.9859 - recall: 0.9824 - f1score: 0.9840 - val_loss: 1.0078 - val_accuracy: 0.8519 - val_precision: 0.8650 - val_recall: 0.8537 - val_f1score: 0.8589\n",
      "Epoch 48/80\n",
      "142/142 [==============================] - 37s 263ms/step - loss: 0.0531 - accuracy: 0.9858 - precision: 0.9824 - recall: 0.9798 - f1score: 0.9810 - val_loss: 0.9769 - val_accuracy: 0.8611 - val_precision: 0.8746 - val_recall: 0.8567 - val_f1score: 0.8650\n",
      "Epoch 49/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.0734 - accuracy: 0.9814 - precision: 0.9833 - recall: 0.9798 - f1score: 0.9814 - val_loss: 0.8884 - val_accuracy: 0.8642 - val_precision: 0.8767 - val_recall: 0.8506 - val_f1score: 0.8627\n",
      "Epoch 50/80\n",
      "142/142 [==============================] - 37s 263ms/step - loss: 0.0944 - accuracy: 0.9779 - precision: 0.9787 - recall: 0.9754 - f1score: 0.9769 - val_loss: 0.9309 - val_accuracy: 0.8457 - val_precision: 0.8676 - val_recall: 0.8354 - val_f1score: 0.8502\n",
      "Epoch 51/80\n",
      "142/142 [==============================] - 37s 263ms/step - loss: 0.0591 - accuracy: 0.9779 - precision: 0.9736 - recall: 0.9718 - f1score: 0.9727 - val_loss: 0.8739 - val_accuracy: 0.8642 - val_precision: 0.8811 - val_recall: 0.8567 - val_f1score: 0.8679\n",
      "Epoch 52/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.0770 - accuracy: 0.9805 - precision: 0.9762 - recall: 0.9701 - f1score: 0.9729 - val_loss: 0.9868 - val_accuracy: 0.8673 - val_precision: 0.8907 - val_recall: 0.8537 - val_f1score: 0.8705\n",
      "Epoch 53/80\n",
      "142/142 [==============================] - 37s 263ms/step - loss: 0.0983 - accuracy: 0.9770 - precision: 0.9710 - recall: 0.9710 - f1score: 0.9710 - val_loss: 1.0651 - val_accuracy: 0.8549 - val_precision: 0.8709 - val_recall: 0.8445 - val_f1score: 0.8566\n",
      "Epoch 54/80\n",
      "142/142 [==============================] - 37s 263ms/step - loss: 0.0683 - accuracy: 0.9823 - precision: 0.9771 - recall: 0.9754 - f1score: 0.9762 - val_loss: 1.1996 - val_accuracy: 0.8549 - val_precision: 0.8731 - val_recall: 0.8415 - val_f1score: 0.8558\n",
      "Epoch 55/80\n",
      "142/142 [==============================] - 37s 262ms/step - loss: 0.0649 - accuracy: 0.9841 - precision: 0.9842 - recall: 0.9842 - f1score: 0.9842 - val_loss: 1.1535 - val_accuracy: 0.8488 - val_precision: 0.8782 - val_recall: 0.8415 - val_f1score: 0.8580\n",
      "Epoch 56/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.0582 - accuracy: 0.9858 - precision: 0.9798 - recall: 0.9771 - f1score: 0.9783 - val_loss: 1.2069 - val_accuracy: 0.8519 - val_precision: 0.8637 - val_recall: 0.8476 - val_f1score: 0.8551\n",
      "Epoch 57/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.0336 - accuracy: 0.9903 - precision: 0.9912 - recall: 0.9903 - f1score: 0.9907 - val_loss: 1.1058 - val_accuracy: 0.8457 - val_precision: 0.8606 - val_recall: 0.8384 - val_f1score: 0.8484\n",
      "Epoch 58/80\n",
      "142/142 [==============================] - 38s 266ms/step - loss: 0.0513 - accuracy: 0.9814 - precision: 0.9815 - recall: 0.9815 - f1score: 0.9815 - val_loss: 1.1458 - val_accuracy: 0.8488 - val_precision: 0.8654 - val_recall: 0.8323 - val_f1score: 0.8478\n",
      "Epoch 59/80\n",
      "142/142 [==============================] - 38s 266ms/step - loss: 0.0668 - accuracy: 0.9876 - precision: 0.9806 - recall: 0.9806 - f1score: 0.9806 - val_loss: 1.1936 - val_accuracy: 0.8488 - val_precision: 0.8590 - val_recall: 0.8415 - val_f1score: 0.8495\n",
      "Epoch 60/80\n",
      "142/142 [==============================] - 38s 265ms/step - loss: 0.0511 - accuracy: 0.9876 - precision: 0.9824 - recall: 0.9815 - f1score: 0.9819 - val_loss: 1.0110 - val_accuracy: 0.8611 - val_precision: 0.8685 - val_recall: 0.8567 - val_f1score: 0.8620\n",
      "Epoch 61/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.0302 - accuracy: 0.9920 - precision: 0.9921 - recall: 0.9912 - f1score: 0.9916 - val_loss: 1.0157 - val_accuracy: 0.8611 - val_precision: 0.8667 - val_recall: 0.8567 - val_f1score: 0.8611\n",
      "Epoch 62/80\n",
      "142/142 [==============================] - 38s 265ms/step - loss: 0.0612 - accuracy: 0.9876 - precision: 0.9886 - recall: 0.9877 - f1score: 0.9881 - val_loss: 1.0693 - val_accuracy: 0.8580 - val_precision: 0.8666 - val_recall: 0.8445 - val_f1score: 0.8544\n",
      "Epoch 63/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.0317 - accuracy: 0.9894 - precision: 0.9894 - recall: 0.9894 - f1score: 0.9894 - val_loss: 1.0260 - val_accuracy: 0.8519 - val_precision: 0.8584 - val_recall: 0.8445 - val_f1score: 0.8510\n",
      "Epoch 64/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.0671 - accuracy: 0.9832 - precision: 0.9780 - recall: 0.9771 - f1score: 0.9775 - val_loss: 1.1127 - val_accuracy: 0.8488 - val_precision: 0.8579 - val_recall: 0.8354 - val_f1score: 0.8440\n",
      "Epoch 65/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.0755 - accuracy: 0.9823 - precision: 0.9842 - recall: 0.9815 - f1score: 0.9827 - val_loss: 1.0987 - val_accuracy: 0.8457 - val_precision: 0.8576 - val_recall: 0.8445 - val_f1score: 0.8506\n",
      "Epoch 66/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.0524 - accuracy: 0.9858 - precision: 0.9868 - recall: 0.9842 - f1score: 0.9854 - val_loss: 1.0549 - val_accuracy: 0.8519 - val_precision: 0.8632 - val_recall: 0.8476 - val_f1score: 0.8546\n",
      "Epoch 67/80\n",
      "142/142 [==============================] - 38s 265ms/step - loss: 0.0776 - accuracy: 0.9796 - precision: 0.9745 - recall: 0.9701 - f1score: 0.9721 - val_loss: 1.1024 - val_accuracy: 0.8333 - val_precision: 0.8506 - val_recall: 0.8323 - val_f1score: 0.8409\n",
      "Epoch 68/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.0586 - accuracy: 0.9867 - precision: 0.9815 - recall: 0.9789 - f1score: 0.9801 - val_loss: 1.0414 - val_accuracy: 0.8457 - val_precision: 0.8611 - val_recall: 0.8384 - val_f1score: 0.8486\n",
      "Epoch 69/80\n",
      "142/142 [==============================] - 38s 265ms/step - loss: 0.0850 - accuracy: 0.9787 - precision: 0.9750 - recall: 0.9692 - f1score: 0.9719 - val_loss: 1.0259 - val_accuracy: 0.8457 - val_precision: 0.8685 - val_recall: 0.8354 - val_f1score: 0.8498\n",
      "Epoch 70/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.0916 - accuracy: 0.9787 - precision: 0.9745 - recall: 0.9727 - f1score: 0.9735 - val_loss: 1.0137 - val_accuracy: 0.8549 - val_precision: 0.8654 - val_recall: 0.8506 - val_f1score: 0.8575\n",
      "Epoch 71/80\n",
      "142/142 [==============================] - 38s 265ms/step - loss: 0.0663 - accuracy: 0.9823 - precision: 0.9839 - recall: 0.9815 - f1score: 0.9826 - val_loss: 0.9626 - val_accuracy: 0.8611 - val_precision: 0.8763 - val_recall: 0.8445 - val_f1score: 0.8591\n",
      "Epoch 72/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.0463 - accuracy: 0.9876 - precision: 0.9877 - recall: 0.9877 - f1score: 0.9877 - val_loss: 1.1000 - val_accuracy: 0.8580 - val_precision: 0.8857 - val_recall: 0.8445 - val_f1score: 0.8633\n",
      "Epoch 73/80\n",
      "142/142 [==============================] - 38s 265ms/step - loss: 0.0673 - accuracy: 0.9796 - precision: 0.9771 - recall: 0.9736 - f1score: 0.9752 - val_loss: 1.0427 - val_accuracy: 0.8457 - val_precision: 0.8521 - val_recall: 0.8354 - val_f1score: 0.8428\n",
      "Epoch 74/80\n",
      "142/142 [==============================] - 38s 265ms/step - loss: 0.0348 - accuracy: 0.9903 - precision: 0.9912 - recall: 0.9903 - f1score: 0.9907 - val_loss: 1.0018 - val_accuracy: 0.8735 - val_precision: 0.8814 - val_recall: 0.8628 - val_f1score: 0.8713\n",
      "Epoch 75/80\n",
      "142/142 [==============================] - 38s 265ms/step - loss: 0.0455 - accuracy: 0.9832 - precision: 0.9780 - recall: 0.9762 - f1score: 0.9771 - val_loss: 1.2065 - val_accuracy: 0.8395 - val_precision: 0.8558 - val_recall: 0.8323 - val_f1score: 0.8433\n",
      "Epoch 76/80\n",
      "142/142 [==============================] - 38s 264ms/step - loss: 0.0407 - accuracy: 0.9911 - precision: 0.9912 - recall: 0.9903 - f1score: 0.9907 - val_loss: 1.0260 - val_accuracy: 0.8580 - val_precision: 0.8624 - val_recall: 0.8476 - val_f1score: 0.8545\n",
      "Epoch 77/80\n",
      "142/142 [==============================] - 38s 265ms/step - loss: 0.0208 - accuracy: 0.9920 - precision: 0.9930 - recall: 0.9921 - f1score: 0.9925 - val_loss: 1.0437 - val_accuracy: 0.8704 - val_precision: 0.8850 - val_recall: 0.8659 - val_f1score: 0.8748\n",
      "Epoch 78/80\n",
      "142/142 [==============================] - 38s 266ms/step - loss: 0.0356 - accuracy: 0.9903 - precision: 0.9912 - recall: 0.9903 - f1score: 0.9907 - val_loss: 0.9155 - val_accuracy: 0.8796 - val_precision: 0.8841 - val_recall: 0.8689 - val_f1score: 0.8760\n",
      "Epoch 79/80\n",
      "142/142 [==============================] - 38s 266ms/step - loss: 0.0631 - accuracy: 0.9832 - precision: 0.9850 - recall: 0.9833 - f1score: 0.9841 - val_loss: 0.9679 - val_accuracy: 0.8488 - val_precision: 0.8659 - val_recall: 0.8476 - val_f1score: 0.8561\n",
      "Epoch 80/80\n",
      "142/142 [==============================] - 38s 265ms/step - loss: 0.0367 - accuracy: 0.9911 - precision: 0.9912 - recall: 0.9903 - f1score: 0.9907 - val_loss: 0.9277 - val_accuracy: 0.8488 - val_precision: 0.8732 - val_recall: 0.8476 - val_f1score: 0.8587\n"
     ]
    }
   ],
   "source": [
    "def recall(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
    "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
    "\n",
    "    # Precision = (True Positive) / (True Positive + False Positive)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1score(y_target, y_pred):\n",
    "    _recall = recall(y_target, y_pred)\n",
    "    _precision = precision(y_target, y_pred)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
    "    \n",
    "    # return a single tensor value\n",
    "    return _f1score\n",
    "\n",
    "\n",
    "\n",
    "model = create_model()\n",
    "#model = ResNet()\n",
    "#model = mobile_net()\n",
    "#model = xception()\n",
    "#model = densenet()\n",
    "#model = resnet()\n",
    "#model = inception()\n",
    "#model =inceptionresnet()\n",
    "\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96)\n",
    "\n",
    "model.compile(keras.optimizers.Adam(lr_schedule), loss = 'categorical_crossentropy', metrics=['accuracy', precision, recall, f1score])\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    training_set, \n",
    "    epochs=80,\n",
    "    validation_data=val_set,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgyx6zeNpwkB",
    "outputId": "5e6dc726-f8ba-4a65-8c4d-e23a017fd0b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "model.save('shoes_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QM1RFDmK9CcS",
    "outputId": "c1c09c30-2857-41f5-f896-7c24e357984d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0  actual y: 79  answer y: 79  prediction: [  0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.00000015   0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.         100.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.        ]\n",
      "index: 1  actual y: 85  answer y: 85  prediction: [  0.           0.           0.           0.           0.\n",
      "   0.           0.00000004   0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.00000001   0.           0.           0.00000004\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.00000001   0.           0.00000001   0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.00000003   0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.00000445   0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      " 100.           0.           0.           0.           0.\n",
      "   0.           0.           0.00000003   0.           0.00000005\n",
      "   0.           0.           0.           0.           0.        ]\n",
      "index: 2  actual y: 0  answer y: 0  prediction: [99.94085     0.          0.00000477  0.00000029  0.00000004  0.0000001\n",
      "  0.          0.00000001  0.0000037   0.00000171  0.00000004  0.00000239\n",
      "  0.          0.          0.00000213  0.00000003  0.00049656  0.\n",
      "  0.00000012  0.00000001  0.00000032  0.00000001  0.00000007  0.00000009\n",
      "  0.00000095  0.00000001  0.00102447  0.00000408  0.00000001  0.\n",
      "  0.01109805  0.00000004  0.00000488  0.00002814  0.00000108  0.00092405\n",
      "  0.00000125  0.          0.          0.00000008  0.00000005  0.00000036\n",
      "  0.02179676  0.          0.00000032  0.          0.00000001  0.00000002\n",
      "  0.00000115  0.          0.          0.00013802  0.00000001  0.000001\n",
      "  0.00000602  0.00000217  0.00000029  0.00000002  0.          0.00000053\n",
      "  0.00000158  0.00002325  0.          0.00007171  0.00000961  0.00000003\n",
      "  0.          0.          0.00000001  0.00000001  0.          0.\n",
      "  0.00000987  0.          0.          0.00000354  0.00027851  0.\n",
      "  0.          0.00013698  0.          0.00000179  0.00000258  0.00000001\n",
      "  0.          0.          0.02218439  0.00000002  0.00000001  0.\n",
      "  0.00000002  0.00000001  0.00040447  0.00012297  0.00000002  0.\n",
      "  0.          0.          0.00000001  0.00037172]\n",
      "index: 3  actual y: 19  answer y: 40  prediction: [ 0.00000336  0.0013187   0.35491154  0.00000532  0.          0.\n",
      "  0.00000002  0.00037664  0.00000008  0.0000125   0.00010374  0.00000055\n",
      "  0.          0.0002015   0.0000036   0.00000017  0.00000004  0.00000025\n",
      "  0.00000105  0.          0.02210894  0.00000001  0.0003284   0.00002251\n",
      "  0.00000094  0.00000005  0.          0.          0.00000004  0.\n",
      "  0.0000961   0.          0.00000225  0.          0.00000667  0.00000004\n",
      "  0.00000001  0.00081699  0.          0.         99.59143     0.\n",
      "  0.00016992  0.0000302   0.00082724  0.00000001  0.          0.02436543\n",
      "  0.          0.00000005  0.          0.00000036  0.00000278  0.00000001\n",
      "  0.00000001  0.00000002  0.00000001  0.00011831  0.          0.00000016\n",
      "  0.00000008  0.00000004  0.          0.00040393  0.          0.00004337\n",
      "  0.          0.          0.          0.00002467  0.          0.\n",
      "  0.          0.          0.00000003  0.          0.00000283  0.00000044\n",
      "  0.          0.00078121  0.0000024   0.          0.00000044  0.\n",
      "  0.          0.          0.00000104  0.00000672  0.00000019  0.00000007\n",
      "  0.00000036  0.00002682  0.00143929  0.00000017  0.00000008  0.\n",
      "  0.00000094  0.00000006  0.          0.00000015]\n",
      "index: 4  actual y: 70  answer y: 70  prediction: [  0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.00000039   0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      " 100.           0.           0.00000001   0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.00000001   0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.00000011   0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.        ]\n",
      "index: 5  actual y: 7  answer y: 7  prediction: [ 0.          0.          0.          0.          0.          0.00000001\n",
      "  0.         99.999985    0.          0.00000012  0.00000002  0.00000001\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.00000563  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.00000001  0.00000002  0.          0.          0.\n",
      "  0.          0.00000002  0.00000012  0.          0.          0.\n",
      "  0.00000004  0.          0.00000002  0.00000211  0.00000002  0.\n",
      "  0.          0.          0.00000028  0.          0.00000261  0.\n",
      "  0.          0.00000029  0.          0.          0.          0.\n",
      "  0.00000002  0.00000016  0.          0.00000019  0.          0.\n",
      "  0.          0.          0.          0.00000003  0.          0.\n",
      "  0.00000001  0.          0.00000154  0.          0.00000144  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.00000001  0.          0.00000001  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "index: 6  actual y: 43  answer y: 43  prediction: [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.]\n",
      "index: 7  actual y: 17  answer y: 20  prediction: [19.761335    0.02810406  0.00148501 20.323946    0.00014303  0.00901129\n",
      "  0.00028514  0.00704168  0.00007424  4.802252    0.00105441  0.00298654\n",
      "  0.00044688  0.00000041  0.0000001   0.00328756  0.00218271  5.9379125\n",
      "  0.00001085  0.00000917 46.465744    0.0000019   0.00005978  0.00000525\n",
      "  0.00034837  0.00073093  0.01249758  0.00033181  0.00000127  0.34929544\n",
      "  0.00058139  0.00000092  0.02060958  0.0001113   0.00000403  0.00058991\n",
      "  0.00000203  0.00000046  0.00000065  0.0000003   0.00128088  0.00000002\n",
      "  1.4281956   0.00005452  0.00002411  0.00005597  0.00007357  0.00392148\n",
      "  0.00001004  0.00000352  0.00002903  0.00113924  0.00000003  0.00493136\n",
      "  0.00000059  0.00011386  0.00000001  0.00464834  0.00208075  0.00001248\n",
      "  0.00000331  0.00000047  0.00000004  0.00000003  0.0000001   0.8044832\n",
      "  0.          0.00000002  0.0000001   0.00000039  0.0000067   0.\n",
      "  0.00000015  0.00000054  0.00000738  0.00000019  0.0000012   0.00000108\n",
      "  0.          0.00000312  0.00184071  0.00000134  0.00004816  0.00000036\n",
      "  0.          0.00000003  0.0000007   0.00009249  0.0043188   0.00000012\n",
      "  0.00000106  0.          0.          0.00000001  0.00033927  0.00001095\n",
      "  0.00000002  0.00976213  0.00000955  0.00000229]\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 1.3882 - accuracy: 0.8540 - precision: 0.8820 - recall: 0.8567 - f1score: 0.8685\n",
      "loss: 1.388, accuracy: 0.854, precision: 0.882, recall: 0.857, f1score: 0.868\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deVzVVfrA8c8joAi4g0uiYmoqlvuS2WLmNJalY1ppm06OZtPetNeU2tT8aszKcpw0bXEqp7RMzaU0Mysr0cwMXBBxBUUEBGXn/P44l1XAe5HLBe7zfr14ce93u8+9F77P95zzPeeIMQallFLeq46nA1BKKeVZmgiUUsrLaSJQSikvp4lAKaW8nCYCpZTycpoIlFLKy2kiUF5BRMJExIiIrxPbThCR76oiLqWqA00EqtoRkVgRyRKR4BLLf3GczMM8E5lStZMmAlVd7QPG5T8RkYuAAM+FUz04U6JRylWaCFR1tRC4o8jz8cD7RTcQkUYi8r6IJIjIfhF5RkTqONb5iMgMETkuIjHA8FL2nS8icSJyWET+ISI+zgQmIp+ISLyIpIjItyLSrci6+iLyiiOeFBH5TkTqO9ZdKiI/iEiyiBwUkQmO5d+IyF+KHKNY1ZSjFHSPiOwB9jiWve44xkkR2SIilxXZ3kdEnhKRvSKS6ljfRkRmi8grJd7LMhF5yJn3rWovTQSquvoRaCgiXR0n6LHAf0ts8wbQCDgfuAKbOP7sWDcJuA7oBfQFxpTY910gB+jo2OZq4C84ZxXQCWgObAU+KLJuBtAHuARoCjwG5IlIO8d+bwAhQE9gm5OvB/AnYAAQ7ni+2XGMpsCHwCci4u9Y9zC2NHUt0BC4EzgNvAeMK5Isg4Ghjv2VNzPG6I/+VKsfIBZ7gnoG+CcwDPgK8AUMEAb4AFlAeJH97gK+cTz+GphSZN3Vjn19gRZAJlC/yPpxwHrH4wnAd07G2thx3EbYC6t0oEcp2z0JfFbGMb4B/lLkebHXdxx/yFniSMp/XWAXMLKM7aKAPzge3wus9PT3rT+e/9H6RlWdLQS+BdpToloICAb8gP1Flu0HWjsenwccLLEuXzvHvnEikr+sTontS+UonbwA3Ii9ss8rEk89wB/YW8qubcpY7qxisYnII8BE7Ps02Cv//Mb18l7rPeA2bGK9DXj9HGJStYRWDalqyxizH9tofC3waYnVx4Fs7Ek9X1vgsONxHPaEWHRdvoPYEkGwMaax46ehMaYbZ3cLMBJbYmmELZ0AiCOmDKBDKfsdLGM5wCmKN4S3LGWbgmGCHe0BjwE3AU2MMY2BFEcMZ3ut/wIjRaQH0BVYWsZ2yotoIlDV3URstcipoguNMbnAx8ALItLAUQf/MIXtCB8D94tIqIg0AZ4osm8c8CXwiog0FJE6ItJBRK5wIp4G2CSSiD15v1jkuHnAAmCmiJznaLQdKCL1sO0IQ0XkJhHxFZFmItLTses24AYRCRCRjo73fLYYcoAEwFdEnsWWCPK9DTwvIp3E6i4izRwxHsK2LywElhhj0p14z6qW00SgqjVjzF5jTEQZq+/DXk3HAN9hGz0XONbNA9YAv2IbdEuWKO4A6gKR2Pr1xUArJ0J6H1vNdNix748l1j8C/IY92Z4AXgLqGGMOYEs2f3Ms3wb0cOzzKra94yi26uYDyrcGWA3sdsSSQfGqo5nYRPglcBKYD9Qvsv494CJsMlAKMUYnplHKm4jI5diSUzujJwCFlgiU8ioi4gc8ALytSUDl00SglJcQka5AMrYK7DUPh6OqEa0aUkopL6clAqWU8nI1rkNZcHCwCQsL83QYSilVo2zZsuW4MSaktHU1LhGEhYUREVHW3YRKKaVKIyL7y1qnVUNKKeXlNBEopZSX00SglFJezm2JQEQWiMgxEdlRxnoRkVkiEi0i20Wkt7tiUUopVTZ3lgjexY4jX5ZrsJN7dAImA3PcGItSSqkyuC0RGGO+xQ6uVZaRwPvG+hFoLCLODPqllFKqEnmyjaA1xUdMPEThpCJKKaWqSI3oRyAik7HVR7Rt2/YsWyulqtrJk+DjA4GBru13/Dj88ANERUH37jBwIDRu7J4Yq0JyMjRqBIUT3xU6cgS++w5SUyEtzf6IQMuWhT+dO5f+GWZnw+efw6BB0MoN9SaeTASHKT6DVCiFs0sVY4yZC8wF6Nu3rw6OpNwqN9f+g9apxvfU5eTAgQNw4gRceCH4+599n8qQmwsJCRAfD7t22RPbd9/B9u0QEAB33QUPPQStWxdu//33sHy5PUnmy8iAiAjYubP48UXgoosgPNy+t/h4++PvDyNHwpgx9mTo41N6fGlpNq4OHUpPKNHR8PPP8Mc/QrNmzr/vzEzYuBHWr4f27eGWW+z7LXrcxx+HTz+FLl1g/Hi4/XY47zy73+zZdl1OTvmvExgIN95o97/8cjh2DObNg7fegsOH4aWX4LHHnI/bWW4ddE5EwoAVxpgLS1k3HDt59rXAAGCWMab/2Y7Zt29foz2LvduxY/DJJ1C/vv2Hbl2JFYoxMfaEc+qU/acbM6b0q7t8xkBkJGzdCr/+Ctu2QVISzJwJVzgz31kJCQn2ZFD0JAOQlQVLlsCHH9oT3b59hSeVunWhTx+49FJ7Rd2rF7RrV37c2dlw6JA9GTZsWHzd6dOwYYM9gR05Ungyjo+38eXlFW4bGAgXX2xPztHRsGiRPUnffjvUq2dPfkeP2hiDgwv38/GxJYBBg2zc4eH288tPLNHREBJir35btrTHWL3aJpDmze1rNmgAQUE2hkOH7Ge/Z4/9Tvz84A9/sN/fkCGwdi28+649NtjEMnYs3HMP9O1b+veQ/31+841NAKdP24uDvDybZO68E269Ff77X3jzTfseJ02CLVvsZycCbdvC/v3Ftw8JKYw7N9e+t/h4+1mvWgX/+58tNYSG2nXZ2XD11XDvvXDttWUnwbMRkS3GmFLerRsTgYh8BAzGTqh9FHgOO2E4xpj/iJ01/E3snUWngT+XMxNVAU0EVScyEj76CJ54wvUif2VLTYUvv4T33oOVK+0/UL4LL7QJoWSROSDA/sMFBdmT0CWXlP9PtHEj3HCDPXbr1rBjh93nlVfsiccYSE+3J/pvv7Unpi+/tP/EYE98F10EiYn2an3mTLjvvsIT8sGD8H//Z0/iV11lY+7WzZ5gPvvMnqi+/tqeUC6/3K4fNMi+37lz7UmhfXvo1w86drQ/DRvaK9zvvoPNm+1JA2z1RM+e9kSU//rG2CQaHQ2xsYWf4fnn220vuMAmtA0b7BWwn1/hibhlS2jRong1RliYPZn7FqlX2LfPfl7z59uT5vDhMHq0PYE1aFDBL98hLc1+FkuW2JJEfvVKWppNDr162ffRpYv9LBYvtu8zX5cuMGGC/Uw//BDef98m/E6div99HztmT8r5OnaEYcPs9zF4MPzyi73CX7LEJuM6dWDiRJg+3X4uYD/j99+3cYwefWYJojz5fw8ff2y/77/+1X4358ojicBdNBFUjb177ZVafDxcdhl88cWZ/8jG2O2+/96eiDZtsv9sL7545h+uMfZqu25de0KpW7fs187NtVdYn3xSeCV6yjFjcatW9mrzjjvsldnq1fbnu+/sVXN52rWDu++2/7RFr07BJphJk+w/3ooV9uT47rvwzDP29Rs2tCecolfDTZvaK7Wrr4YBA+x79vWFlBQb37JlcNttMG0avPaaLd4bY4+9a5c9RuvWdvu0NPvat91mH69ZYxMx2BP58OH26vXqq8uussrIsFew+Vey27YVJqmiMXfqZE9u7dvb5JK/bXS0/f6GDbM/l11mS10VkZpqP4uK7l8ZjClMbIMGQf/+xUtJKSn2e1+/vvj32qgR9Ohhk0qPHmf+reSLi4OlS+3/yUUXufe9VAZNBMolR47YP+6UFFsf+fTT9kS3alVhNcL69fDII/YfDWzRt39/mwzS0+1VzLPP2pPzBx/Yk+rvvxe+RtOm0KaNvVKaOLGwvjYy0j7/8UdbZO/UqfAKtEcPeyXtW0rLVlaWvYrNl5dn40hLs0lk1y74z39s3PXqwdCh9kSRlmbf56+/2iqExYuhSZPC46Sl2au/+PjC0kVQkL0q79On7BJGXp5NiM8+a1/Hxwf+/GebWNq1s6WDNWvgq6/s8caPt5950ZP8gQO2IbV/f5s83C0725YCVO2kiUA57cQJWy2xf7+tpujXzxaBx46F3r1h1ix7glu2zFY7PPqoLS6Hh9uT2NGj8NxztoErIMAWc/PybN31LbfYk3D+Vf727fZKPr++NjQUXn7ZljxmzYJx48qv566I33+3J/Zvvy2sOgoMtO/tmWcq/0S4erWtzrj/fnsVrpSnaCLwUsbAb7/ZE2v79mff/tgxGDHCVhOsWgVXXlm47vPP7d0M2dn2eE89BQ88UHbRf8cOmDHD3jUxfry9La6s7WbPhoUL7ZX7TTfBG2/YOl+lVOXRRFCLHDliT5xXXmnvSintCjYuzt7J8O67tqqlTh3bYPX447Y6o6TcXFt//fTT9mT88cfwpz+dud3atbBunb09sLJP1Ckp9s6Pbt0q97hKKUsTQS1y553wzjv2cdOm9lbHAQNsVc7evfb2uV9/LayOueMOu+7f/7adfoYOtXc/5N8FkpdnE8CWLbaOfPZs22ColKpdNBHUEgcO2I4yd95pb8dbvNjW1ef36mzf3tZD9+1r7z4pWh2TkmKv+l9/vfitcWDvxJk5E26+ufLr5JVS1YMmglri/vthzhx75Z8/0kZmpq0Kat3auYZOY+ytffkNtklJtpqpZKcipVTtUl4iqBFjDanCrua3316YBMDehRMW5vxxROxJv2HDyumkopSq+arxaCreKTfX3joZFVV8+Wuv2av/xx/3TFxKqdpLE0E18/TT9rbM/v3tOC1g6/dnz7bjppR1G6ZSSlWUVg1VI4sX24HObrvN3v0zejQ8+aTt+HTypH2slFKVTRNBNfH773ZArIED7YBdxtgBy/75T7v+mmvsoFpKKVXZtGqoGkhOhlGjbI/dxYvtgGz16tkRJ+fOtXcETZ3q6SiVUrWVlgg8LC/P3gm0b58dEO2884qvnzTJ/iillLtoIvCwZ56xwx7Pnm1Hn1RKqaqmVUMe9OGHtg3grrvsOPlKKeUJmgg8ZPNmO+7+5ZfbfgM6tINSylO0aqgK/Pe/tk9A9+52cpU2bexgcS1b2rH+y5utSyml3E0TgZvt2weTJ9u7gD7/vHBKvKAgO0NVWdPgKaW8R1xqHG/8/AYXNb+IcReNq/LX10TgRsbYKRt9fOxsXM2a2YlYfv3V9hyuCfOcKqWck5OXw5roNfyhwx+o6+NcMf9gykFe+v4l3t76Npm5mfj7+jMgdADnN6mCuUmL0DYCN1q0yE5V+OKLtjooIMAmgEmTbBWRUp6Sm5dLbHIs2bnZng6l1nhx44tc99F1vLjxxbNum5aVxsNrHqbDrA68teUtbut+G99O+BbfOr7cs/IeqnpUaB2G2k1OnICuXe3IoD/8UPYk56pmS8tKY8EvCxjfYzyN/BtV+evnmTwOphwk6ngUkQmRNKzXkDt73UkdOfMab9GORSzduZSo41HsOr6LzNxMwkPCmT9iPheHXlzlsdcmW45s4eL5F+NXxw8RYc99ezivwXmlbrs2Zi2Tlk8iNjmWv/T6C89c/gztGrcD4LUfX+OhNQ/xyY2fMCZ8TKXGqPMReMCkSXYmsS1b9Oq/tsrJy2HkopGs3LOSO3veyfyR88/Y5lTWKXYe30nvVr2RSrw1bH/yfl76/iUWbl9IWlZasXWTe09mznVziiWDl79/mcfXPk6bhm3o3qI74SHhtApqxcwfZ3L45GEevPhBnr/yeQLrBlZajKUxxvDRjo/4+/q/k5KRUrDct44vHZt2JDwknPCQcLq36M5lbS/Dz8eJSTbcICohiiVRS/hs52ekZqYWxNU1uCvDOg4jJDCkYNuMnAz6zO1DckYyy8ctZ+D8gdx60a0sGLmg2DFTMlJ4eM3DLNi2gE5NOzF/xHwua3dZsW1y8nLoN68fx04dY+c9O2lQr0GlvSdNBFVswwYYPBgefRReftnT0Sh3MMYwZcUU5m6dy4DWA/jp8E989+fvGNR2UME2uXm5XPvhtXy590suDr2Yv1/+d67peM05JYToE9H8c+M/eX/7+wjCuIvGcUnoJfYkFdKVVze9yovfvciEnhN4+/q38anjwz++/Qd/X/93xl44loWjFuJbp7Bp8GTmSZ5Y+wRzIuYQ1jiMPq0KJ7Wu51uPq9pfxcjOI2kW0OyM959n8vCp43xR92DKQe7+4m6+2PMF/c7rR//W/QvWZeRksDtxN5EJkSSmJwLQxL8JIzqPYHTX0fyhwx/w9/V36nXyTF6pJaKzSctK462It1iwbQGRCZEADAwdyHkNziPqeBS7E3eTk5dD0/pNee2Pr3Fb99sQER758hFe2fQKq25dxbCOw3j0y0d5ZdMrbL1rKz1b9gTg+OnjXL3warYf3c4jlzzCc1c8R32/+qXG8dOhnxg4fyAPDHiAV4e96vL7KIsmgip06pQtAeTlwW+/QaB7L7A8KjIhkqT0pGInP2/x4sYXefrrp3ny0id5+rKnCf93OA3rNWTr5K0FV7FTv5nKtA3TmNhrImtj1rI/ZT99WvVh+pXTubbTtWcc0xjD6ujVdGvejbaN2p6xfunOpdz4yY34iA+Tek/isUGP0aZRmzOOMX3DdKZumMotF91C+8bteWHjC9ze/XbeGflOmSfub/d/y9NfP01SelLBsqSMJI6kHsFHfBgcNpjL2l5GbEosUQm2GirP5DH8guGM7jqaaztdS1DdoIISUGRCJCczTxYcKzE9kRk/zCDX5PLCkBe4r/99ZcaScCqBTYc2sSRqCct2LSM5IxnfOr50atqJriFdCQ8OL0h8nZt1pr5ffXs1vms5i6MW8+XeL5ncezKvDnvVqYRwMvMkb/78JjM3zSQxPZFL217Kzd1uZlSXUbRu2Lpgu+zcbLbFb+OB1Q+w6dAmrul4Dbd3v51bP72VyX0m85/r/gNAckYyHWd1pEfLHqy9fS0JpxMY+v5Qdifu5rObP+OaTtecNaYpK6Ywb+s8/nnVPwn0KzyJXBF2BRc2v/Cs+5dGE0EVeuAB20Hsm2/giis8HY37RCZEMmjBIJIzkpnQcwKvXP0KTes39XRYgD2p/euHf9EtpBuju46m73l9i12Fp2amkpieSJuGbc44GRljOHrqKCfST5R5/I37NzLliyncctEtLBy1kDpSh2W7ljFy0UheHvoyjw56lNXRq7n2g2u5o8cdvDPyHXLycli4fSEvbnyRvUl7ubnbzcy6ZhbNA5sDcCDlAFNWTGFV9CpaBbVi3R3r6BrSteA1Nx3cxJD3h9CjRQ8+u/kzWjVoVe5nkJ+oACb2mshb173l0tV7/mexNW4rS6KWsDhyMXtO7KFlUEu6BnclPCScrNwsPt/1OcdOHcPf15/mgc05kHKgzOMNaT+EedfPc+mOmKzcLNbvW8+G/RuIOh5FVEIU0SeiyTW5AAhCu8btOHzyMNl52YQ2DKVbSDfW7F1zRhWZMYZPIj9h1k+zyMrNKniNPSf2kJyRzLWdruWZy55hYJuB5caUm5fLmz+/yVNfP8Xp7NOc3+R8fp3yK0F1gwq2eeOnN7h/9f3Mu34eMzfNJDY5lmXjljH0/KFOve+k9CT6zO3DvuR9xZbPGT6HKX2nOHWMkjQRVJH8KqH77rPJoLaKS43j4vkXk5WbxS0X3sLrP71OcEAw/x7+b27oesM5Hz85I5kVu1fw29Hfii1v26gto7qOKrMR7mTmSZ5c+yT/jvg3IQEhJGUkkZOXQ9tGbbmq/VUcTj1MVEIUB08eBKCeTz06B3cmPCScQL/AggbX5Izks8Y4OGwwq29dTT3fegXLRi4aydqYtXx525eMXDSS8xqcx49/+ZEAv4CCbbJzs3np+5d4/tvnCaobxOvDXic1M5XH1j5GnsnjsUseY07EHAyGdXes48LmF7IncQ8D5w+ksX9jNk3cVKx+ujzztswjPi2epy9/ukJVJUUZYzidffqMNoTcvFy+O/Adn0Z9yvH048Wu1pvVL6xOqiN1aFq/aaW0k2TmZLLnxJ6CksnOxJ20adiG0V1H0691PwThma+fKVZFdvTUUe5ZeQ9Ldy6la3BXwhqHFRwvJDCE+/vfT5/z+pT9oqXYl7SPFze+yJS+U87YNzs3mwvnXMjuxN0E+gXyxS1fcEWYa1eGWblZxdpRAILqBpVZpXQ2mgiqwKlTtuewiO0nUFurhFIzU7ni3SvYnbibb//8Lb1b9eaXuF+4c9mdbIvfxqA2g7gx/EZu6HrDGdUW5TmdfZpFOxaxOHIxa2PWkp2XjV8dv4KrWGMMmbmZAFzS5hJGdx1NeEh4wf4JpxJ4+uunOXTyUEHDZ2ZuJst2LWNJ1BI2HdxEWOOwgqqF4IBgWyd9PJKohChOZZ8quNLtGtyV5oHNyzxp+dXx448d/1jsBA+2ATf83+Fk5GQQ6BdIxOQILmhW+sTQkQmRTFw2kR8P/QjA0POHMve6ubRv0p5dx3cx5P0hZOZk8tHoj7j7i7tJyUzhhzt/oFOzTk5/pt6saBXZVe2vIuJIBJm5mUwfPJ2HBj5UrJ3EXb7a+xX3rbqP+SPmV4vqU00EVeC+++DNN22p4PLLPR2Ne2TnZjNi0Qi+2vsVy8ctL1bXmZ2bzZs/v8mCbQvYcWwHAANaD+DStpcWnmBDutLYv/EZx12/bz1/Wf4XYpJiCGscxpiuYxgdPpr+rfsXu5LNv5NjSdQStsVvO+M41eFWyBk/zODRrx516va/3Lxc3t76NoF1A7n1oluLJZ7oE9EMeW8IB08exN/Xn/Xj1+stnhWQX0V2RbsrmHf9PK9OpJoI3Ozbb217wAMP2Enma6L07HSmbZjGvK3zuO6C63jq0qfoHGwnSDbGsCp6FdM3TOenwz8x7/p5/KX3X8o81u7E3SyJXMLSXUvZfnQ7GTkZBesGtB7A6K6jGR0+mmb1m/HYV48xd+tcOjbtyH+G/4ch7Yc4VX0QmxxLfFp8wXMf8aFHyx5O9+h0p6NpR2kR1OKcjxObHMtdK+7i3n73cn3n6yshMu+0L2kf7Rq3O+fqsZpOE4EbZWRAz56QlWWHjwgIOPs+58IYw/6U/cUauwL8AmjdoHWxE+jJzJOs2L2Cz3d9jr+vPzd0uYGrO1xdav3ixv0bmbhsIntO7GHo+UP5/sD3ZOZmclO3mxjWYRhv/PwGW+K20K5RO6ZfOZ07etzhdLz5PVgjEyL5Jf4XPt/1OVvjtgIQ6BdIek46fxv4N6YOnnpGVYtSqvJoInCj556D6dPtAHJXX+3e1zqdfZqJyyayaMeiM9YF+gXa+u+QcBJPJ/JVzFdk5WbRKqgVGTkZJGUkEegXyPALhtOhSYeC/Q6ePMh/t/+XsMZhzLt+HkPPH8rRtKPM3DSTf0f8m7SsNDo06cBTlz3F7d1vr5QOPvuS9vFp1KdsO7qN+/vfT7/W/c75mEqp8nksEYjIMOB1wAd42xjzfyXWtwXeAxo7tnnCGLOyvGNWp0QQFWX7DNx4I3zwgXtf60DKAf606E9si9/Gk5c+Sbfm3QrWpWSk2Hu3j0cSmRBJXZ+6jOoyijHhY7g49GJy83L5JvYbFkcuZtnuZSSeTizY18/Hj0m9J/HCkBfOuCMk8XQivx37jUvbXloljWtKKffxSCIQER9gN/AH4BCwGRhnjIksss1c4BdjzBwRCQdWGmPCyjtudUkEeXn2VtEdO2DnTmjevOLHMsYU3Bddmh8O/sCYj8eQmZvJhzd8yPALhlf8xZRSXqm8RODOy7z+QLQxJsYRxCJgJBBZZBsDNHQ8bgQccWM8lWr+fNi40f4+lyRwMvMkly64lN+O/Vbudhc0u4DPx35Ol+AuFX8xpZQqhTsTQWvgYJHnh4ABJbaZCnwpIvcBgUCp3e5EZDIwGaBt2zO73le1xER47DF7p9Cf/3xux3po9UP8nvA7T136VJkdRQL8Ariz152l3nqplFLnytMVv+OAd40xr4jIQGChiFxojMkrupExZi4wF2zVkAfiLGbxYkhOhpkzz22u4eW7lrNg2wKeuvQpXrjqhcoLUCmlXODOG2sPA0W7loY6lhU1EfgYwBizCfAHqv3kjUuWQMeO0KtXxY9x/PRxJi2fRPcW3Xlu8HOVF5xSSrnInYlgM9BJRNqLSF1gLLCsxDYHgKsARKQrNhEkuDGmc3biBKxfD6NHn1tp4J6V93Ai/QQLRy2sFp2glFLey21VQ8aYHBG5F1iDvTV0gTHmdxGZDkQYY5YBfwPmichD2IbjCaaad2xYtgxycmwicFVOXg4xSTEs27WMj3//mBeHvEj3Ft0rP0illHKBdihz0fXX24noY2OdLxEsjlzMtA3T2J24u6BH8GVtL+Pr8V/r/flKqSrhqdtHa52TJ+HLL+Gvf3U+CaRkpDB5+WRaBrXkwQEPFkx516NlD00CSqlqQc9ELvjiCzum0BgX5pR+7cfXSMpIYu0da+ndqrf7glNKqQry7uH4XLRkCbRqBQPLn8CowIn0E8z8cSY3dL1Bk4BSqtrSROCk06dh1SoYNQrqOPmpzfhhBqmZqUwbPM29wSml1DnQROCk1attMnD2bqGEUwnM+mkWN194c4Unm1ZKqaqgicBJS5ZAs2bOzz720vcvkZ6TztQrpro1LqWUOleaCJyQnQ0rVsCf/gS+TjSvH0k9wuzNs7mt+20Fs3wppVR1pYnACdHR9tbRwYOd2/69be+RkZPBs5c/69a4lFKqMmgicEKkY+Ds8HDntt+ZuJPQhqF0aNrh7BsrpZSHaSJwQlSU/d3ZyVqemKQYzm9yvvsCUkqpSqSJwAmRkRAWBoGBZ90U0ESglKpZNBE4ISrK+Wqh9Ox0jqQeKTZBvFJKVWeaCM4iN9fOSdy1q3PbxybHAmiJQClVY2giOIvYWMjIcL5EsDdpL6CJQClVc2giOIv8hmJnSwQxSTGAJgKlVM2hieAs8m8ddSURBPoFEhIQ4r6glFKqEmkiOIuoKDviaOPGzm0fkxRDh6YdkHOZx1IppaqQJmUd7AAAABxVSURBVIKziIx0vn0A9NZRpVTNo4mgHMbYEoGz1ULGGJsIGmsiUErVHJoIynH4MKSmOl8iiE+LJz0nXUsESqkaRRNBOfSOIaWUN9BEUA5XB5vLTwQ62JxSqibRRFCOyEho2hRCnLwTdG/SXgShXaN27g1MKaUqkSaCcuSPMeTsnaAxSTGENgylnm899wamlFKVSBNBOSIjnW8fAL11VClVM2kiKENCAiQmah8CpVTtp4mgDK4OLXE6+zRxaXE6/LRSqsbRRFCG/FtHnS0R7EvaB+ito0qpmkcTQRkiIyEoCEJDndte+xAopWoqTQRlyB9awpU7hkATgVKq5tFEUAZXxhgCmwiC6gYRHBDsvqCUUsoNNBGUIj3djjPUsaPz++xN2kuHJjr8tFKq5nFrIhCRYSKyS0SiReSJMra5SUQiReR3EfnQnfE4a/9++/t8F2p59NZRpVRN5bZEICI+wGzgGiAcGCci4SW26QQ8CQwyxnQDHnRXPK6IsdX9tG/v3PZ5Jo99yfs0ESilaqSzJgIRuV5EKpIw+gPRxpgYY0wWsAgYWWKbScBsY0wSgDHmWAVep9LlJwJnSwTxafFk5GRoIlBK1UjOnOBvBvaIyMsi0sWFY7cGDhZ5fsixrKgLgAtE5HsR+VFEhpV2IBGZLCIRIhKRkJDgQggVs28f1K8PLVo4t73eMaSUqsnOmgiMMbcBvYC9wLsisslxYm5QCa/vC3QCBgPjgHkicsbswMaYucaYvsaYviHODgV6DmJibLWQs+2+scmxAIQ1DnNbTEop5S5OVfkYY04Ci7HVO62AUcBWEbmvnN0OA22KPA91LCvqELDMGJNtjNkH7MYmBo/at8+1huK41DgAzmtwnpsiUkop93GmjWCEiHwGfAP4Af2NMdcAPYC/lbPrZqCTiLQXkbrAWGBZiW2WYksDiEgwtqooxsX3UKmMsSUClxJBWhwBfgE0qFsZhSSllKpavk5sMxp41RjzbdGFxpjTIjKxrJ2MMTkici+wBvABFhhjfheR6UCEMWaZY93VIhIJ5AKPGmMSK/pmKkNiop2n2Nk7hsA2FrcKaqV9CJRSNZIziWAqEJf/RETqAy2MMbHGmHXl7WiMWQmsLLHs2SKPDfCw46da2GfHjnO5RNAyqKV7AlJKKTdzpo3gEyCvyPNcx7JaydVbR8G2EbRq0Mo9ASmllJs5kwh8Hf0AAHA8ruu+kDwrPxGEhTm/T3xaPC0DtUSglKqZnEkECSIyIv+JiIwEjrsvJM/atw+aN7dDUDsjPTudlMwULREopWosZ9oIpgAfiMibgGA7id3h1qg8yNU7huLT4gFoFaSJQClVM501ERhj9gIXi0iQ43ma26PyoJgYuPhi57ePS7Pt6NpYrJSqqZwpESAiw4FugH/+LZLGmOlujMsjcnLgwAG45Rbn98nvTKZVQ0qpmsqZDmX/wY43dB+2auhGoJ2b4/KIgwchN7diVUNaIlBK1VTONBZfYoy5A0gyxkwDBmJ7ANc6+X0IXOlMFpcWRx2pQ0iA+8dAUkopd3AmEWQ4fp8WkfOAbOx4Q7VORfoQxKfF0yKwBT51fNwTlFJKuZkzbQTLHSOC/gvYChhgnluj8pCYGPD1hdBQ5/fRXsVKqZqu3ETgmJBmnTEmGVgiIisAf2NMSpVEV8X27YN27cDHhYt77VWslKrpyq0aMsbkYaebzH+eWVuTALjehwC0V7FSquZzpo1gnYiMFi8YWtPVRJCbl8vRU0e1RKCUqtGcSQR3YQeZyxSRkyKSKiIn3RxXlUtNhePHXbtj6Pjp4+SZPG0jUErVaM70LPaK2VYqOvw06PASSqma7ayJQEQuL215yYlqarqKDj8N2qtYKVWzOXP76KNFHvsD/YEtwBC3ROQhFelMpr2KlVK1gTNVQ9cXfS4ibYDX3BaRh8TEQKNG0KSJ8/vogHNKqdrAmcbikg4BXSs7EE+LibGlAVfujYpPi6dhvYYE+AW4LzCllHIzZ9oI3sD2JgabOHpiexjXKocPQ9u2ru0TlxanDcVKqRrPmTaCiCKPc4CPjDHfuykejzl6FPr3d20f7VWslKoNnEkEi4EMY0wugIj4iEiAMea0e0OrOrm5cOwYtHSxqj8+LZ5+rfu5JyillKoiTvUsBuoXeV4fWOuecDzj+HHIy3MtERhjtGpIKVUrOJMI/ItOT+l4XKtaR+PtXaAuJYK0rDROZ5/WO4aUUjWeM4nglIj0zn8iIn2AdPeFVPUqkgi0V7FSqrZwpo3gQeATETmCnaqyJXbqylqjQolAexUrpWoJZzqUbRaRLkBnx6Jdxphs94ZVtfITQYsWLuyjvYqVUrWEM5PX3wMEGmN2GGN2AEEi8lf3h1Z14uOhQQMIDHR+H60aUkrVFs60EUxyzFAGgDEmCZjkvpCqXnx8xW4d9avjR9P6Td0TlFJKVRFnEoFP0UlpRMQHqOu+kKpeRRJB/lzFXjBfj1KqlnMmEawG/iciV4nIVcBHwCr3hlW1KpQIUnXSeqVU7eBMIngc+BqY4vj5jeIdzMokIsNEZJeIRIvIE+VsN1pEjIj0dea4la2iVUN6x5BSqjY4ayJwTGD/ExCLnYtgCBB1tv0cVUizgWuAcGCciISXsl0D4AHHa1S5jAxITnbtjiHQAeeUUrVHmYlARC4QkedEZCfwBnAAwBhzpTHmTSeO3R+INsbEGGOygEXAyFK2ex54CchwOfpKcPSo/e1KiSA7N5vjp49r1ZBSqlYor0SwE3v1f50x5lJjzBtArgvHbg0cLPL8kGNZAUeP5TbGmC/KO5CITBaRCBGJSEhIcCGEs6tIZ7Kjp2z20BKBUqo2KC8R3ADEAetFZJ6jobjSbpERkTrATOBvZ9vWGDPXGNPXGNM3JCSkskIAzq1XsZYIlFK1QZmJwBiz1BgzFugCrMcONdFcROaIyNVOHPsw0KbI81DHsnwNgAuBb0QkFrgYWFbVDcbnNM6QNhYrpWoBZxqLTxljPnTMXRwK/IK9k+hsNgOdRKS9iNQFxgLLihw3xRgTbIwJM8aEAT8CI4wxEaUfzj3yE0Hz5s7vUzDOkFYNKaVqAZfmLDbGJDmqaa5yYtsc4F5gDfYuo4+NMb+LyHQRGVGxcCtffDwEB4Ofn/P75JcIWgS5eKuRUkpVQ86MPlphxpiVwMoSy54tY9vB7oylLBXtTBYcEExdn1rVwVop5aVcKhHURhUdXkKrhZRStYUmgoomAm0oVkrVEl6dCIyxHcoqUjWkJQKlVG3h1YkgNRXS011LBHkmj6OnjmoiUErVGl6dCCrShyDxdCI5eTlaNaSUqjU0EaCT1iulvJsmAlwbeVQnrVdK1TaaCNASgVLKu3l9IvD1haYuTDusJQKlVG3j9YmgRQuo48KnEJcWR8N6DQnwC3BfYEopVYW8PhFor2KllLfTRFCRzmRaLaSUqkU0EWiJQCnl5bw2EeTmwrFjriUCY4wOL6GUqnW8NhEkJtpk4EoiOJl5kvScdK0aUkrVKl6bCLQPgVJKWZoIKjBpvZYIlFK1iSYCLREopbycJgItESilvJxXJ4KAAAgKcn6fuLQ4/H39aVSvkfsCU0qpKua1ieDoUddGHYXCPgQi4p6glFLKA7w2ESQkQEiIa/tor2KlVG2kicAF2qtYKVUbaSJwgfYqVkrVRl6ZCIxxPRGkZ6eTkpmiVUNKqVrHKxNBaipkZUHz5s7vo30IlFK1lVcmgoQE+9uVEoH2IVBK1VaaCJwUn2Z7oGmJQClV22gicFJB1ZCWCJRStYxXJoJjx+xvV6uGfOv4EhwQ7J6glFLKQ7wyEVS0RNAisAV1xCs/MqVULeaVZ7WEBKhfHwIDnd8nLk17FSulaie3JgIRGSYiu0QkWkSeKGX9wyISKSLbRWSdiLRzZzz5tDOZUkoVclsiEBEfYDZwDRAOjBOR8BKb/QL0NcZ0BxYDL7srnqJ0eAmllCrkzhJBfyDaGBNjjMkCFgEji25gjFlvjDntePojEOrGeAq4mghy8nJIOJWgVUNKqVrJnYmgNXCwyPNDjmVlmQisKm2FiEwWkQgRiUjIb+k9B64mggMpBzAY2jZqe86vrZRS1U21aCwWkduAvsC/SltvjJlrjOlrjOkb4mqdTikSElwbXiL6RDQAHZt2POfXVkqp6sbXjcc+DLQp8jzUsawYERkKPA1cYYzJdGM8AJw6BenprpUI9p7YC2giUErVTu4sEWwGOolIexGpC4wFlhXdQER6AW8BI4wxx9wYS4GK9CGIPhFNfd/62lislKqV3JYIjDE5wL3AGiAK+NgY87uITBeREY7N/gUEAZ+IyDYRWVbG4SpNhRJBUjQdmnbQKSqVUrWSO6uGMMasBFaWWPZskcdD3fn6panI8BLRJ6K5oNkF7glIKaU8rFo0FlclV0sEeSaPvSf20rGJtg8opWonTQRncST1CJm5mdpQrJSqtbwyEdStCw0aOLd9/q2jHZp2cGNUSinlOW5tI6iO8juTOdvuq30IlHKf7OxsDh06REZGhqdDqTX8/f0JDQ3Fz8/P6X28NhE4K/pENH51/GjTsM3ZN1ZKueTQoUM0aNCAsLAwvSuvEhhjSExM5NChQ7Rv397p/byyasjVRHB+k/PxqePjvqCU8lIZGRk0a9ZMk0AlERGaNWvmcgnLKxOBK8NL7E3aq9VCSrmRJoHKVZHP0ysTgbMlAmMM0Sei6dBEG4qVUrWXVyWCjAxIS3M+ERw7dYy0rDQtEShVSyUmJtKzZ0969uxJy5Ytad26dcHzrKyscveNiIjg/vvvr6JI3curGotd7UOgdwwpVbs1a9aMbdu2ATB16lSCgoJ45JFHCtbn5OTg61v6abJv37707du3SuJ0N00E5dBEoFTVefBBcJyTK03PnvDaa67tM2HCBPz9/fnll18YNGgQY8eO5YEHHiAjI4P69evzzjvv0LlzZ7755htmzJjBihUrmDp1KgcOHCAmJoYDBw7w4IMP1qjSglclAlfHGdqbtJc6Uod2jatkKmWlVDVx6NAhfvjhB3x8fDh58iQbN27E19eXtWvX8tRTT7FkyZIz9tm5cyfr168nNTWVzp07c/fdd7t0L78neVUiqEiJoF2jdtT1qeu+oJRSgOtX7u5044034uNjbxlPSUlh/Pjx7NmzBxEhOzu71H2GDx9OvXr1qFevHs2bN+fo0aOEhlbJ7LvnzKsaiyuSCLRaSCnvExgYWPD473//O1deeSU7duxg+fLlZd6jX69evYLHPj4+5OTkuD3OyuJ1icDXFxo3dm57TQRKqZSUFFq3ttOtv/vuu54Nxk28LhEEBzs3ztCJ9BMkZSRpHwKlvNxjjz3Gk08+Sa9evWrUVb4rxBjj6Rhc0rdvXxMREVGhfUeOhH37YPv2s2+7+fBm+r/dn6U3L2Vkl5EVej2lVPmioqLo2rWrp8OodUr7XEVkizGm1Ptdva5E4OzwEnrrqFLKW3hdInC1D8H5Tc53Y0RKKeV5mgjKEJ0UTWjDUOr71XdvUEop5WFekwiysiAlxYXOZCf2akOxUsoreE0iOH7c/nYmEaRkpLD96HY6N+vs3qCUUqoa8JpE4MrwEq/++CqpWanc1fcu9wallFLVgNckAmd7FZ9IP8GrP77KqC6j6N2qt/sDU0p5zJVXXsmaNWuKLXvttde4++67S91+8ODB5N++fu2115KcnHzGNlOnTmXGjBnlvu7SpUuJjIwseP7ss8+ydu1aV8OvNJoISpjxwwxSM1OZNnia+4NSSnnUuHHjWLRoUbFlixYtYty4cWfdd+XKlTR2dpiCEkomgunTpzN06NAKHasyeM2gc84kgoRTCcz6aRY3dbuJi1pcVDWBKaUAeHD1g2yLr9xxqHu27Mlrw8oezW7MmDE888wzZGVlUbduXWJjYzly5AgfffQRDz/8MOnp6YwZM4Zp0868MAwLCyMiIoLg4GBeeOEF3nvvPZo3b06bNm3o06cPAPPmzWPu3LlkZWXRsWNHFi5cyLZt21i2bBkbNmzgH//4B0uWLOH555/nuuuuY8yYMaxbt45HHnmEnJwc+vXrx5w5c6hXrx5hYWGMHz+e5cuXk52dzSeffEKXLl0q5XPymhLBBRfA7bdD06Zlb/PS9y+RnpPO1MFTqywupZTnNG3alP79+7Nq1SrAlgZuuukmXnjhBSIiIti+fTsbNmxgeznDEWzZsoVFixaxbds2Vq5cyebNmwvW3XDDDWzevJlff/2Vrl27Mn/+fC655BJGjBjBv/71L7Zt20aHDoV3J2ZkZDBhwgT+97//8dtvv5GTk8OcOXMK1gcHB7N161buvvvus1Y/ucJrSgS9LztKcujXiIwFzhxsKC41jtmbZ3Nb99voElw5WVYp5bzyrtzdKb96aOTIkSxatIj58+fz8ccfM3fuXHJycoiLiyMyMpLu3buXuv/GjRsZNWoUAQEBAIwYMaJg3Y4dO3jmmWdITk4mLS2NP/7xj+XGsmvXLtq3b88FF1wAwPjx45k9ezYPPvggYBMLQJ8+ffj000/P+b3n85oSwRs/v8Etn97C9R9dz6GTh4qtO5J6hLu/uJvs3GyevfxZD0WolPKEkSNHsm7dOrZu3crp06dp2rQpM2bMYN26dWzfvp3hw4eXOfT02UyYMIE333yT3377jeeee67Cx8mXP9R1ZQ9z7TWJYNrgacy8eiZf7/ua8NnhvBXxFrHJsdzzxT2c//r5rNi9gmmDp9GhqXYiU8qbBAUFceWVV3LnnXcybtw4Tp48SWBgII0aNeLo0aMF1UZlufzyy1m6dCnp6emkpqayfPnygnWpqam0atWK7OxsPvjgg4LlDRo0IDU19Yxjde7cmdjYWKKj7RA3Cxcu5Iorrqikd1o2r6ka8qnjw0MDH2Jkl5FMWj6JKV9MAcCvjh8Tek7giUuf0HGFlPJS48aNY9SoUSxatIguXbrQq1cvunTpQps2bRg0aFC5+/bu3Zubb76ZHj160Lx5c/r161ew7vnnn2fAgAGEhIQwYMCAgpP/2LFjmTRpErNmzWLx4sUF2/v7+/POO+9w4403FjQWT5kyxT1vugivGoY6nzGG9399nz0n9jC5z2TaNmpbSdEppVyhw1C7h6vDULu1RCAiw4DXAR/gbWPM/5VYXw94H+gDJAI3G2Ni3RmT43UZ33O8u19GKaVqBLe1EYiIDzAbuAYIB8aJSHiJzSYCScaYjsCrwEvuikcppVTp3NlY3B+INsbEGGOygEVAyam+RgLvOR4vBq4ScWYiSaVUbVHTqqeru4p8nu5MBK2Bg0WeH3IsK3UbY0wOkAI0K3kgEZksIhEiEpGQ30VYKVXj+fv7k5iYqMmgkhhjSExMxN/f36X9asRdQ8aYucBcsI3FHg5HKVVJQkNDOXToEHqBV3n8/f0JDQ11aR93JoLDQJsiz0Mdy0rb5pCI+AKNsI3GSikv4OfnR/v27T0dhtdzZ9XQZqCTiLQXkbrAWGBZiW2WAfm374wBvjZaRlRKqSrlthKBMSZHRO4F1mBvH11gjPldRKYDEcaYZcB8YKGIRAMnsMlCKaVUFXJrG4ExZiWwssSyZ4s8zgBudGcMSimlylfjehaLSAKwv4K7BwPHKzGcylRdY6uucUH1ja26xgXVN7bqGhfUntjaGWNKnZGlxiWCcyEiEWV1sfa06hpbdY0Lqm9s1TUuqL6xVde4wDti85rRR5VSSpVOE4FSSnk5b0sEcz0dQDmqa2zVNS6ovrFV17ig+sZWXeMCL4jNq9oIlFJKncnbSgRKKaVK0ESglFJezmsSgYgME5FdIhItIk94OJYFInJMRHYUWdZURL4SkT2O3008EFcbEVkvIpEi8ruIPFAdYhMRfxH5WUR+dcQ1zbG8vYj85PhO/+cYysQjRMRHRH4RkRXVJTYRiRWR30Rkm4hEOJZ5/O/MEUdjEVksIjtFJEpEBno6NhHp7Pis8n9OisiDno6rSHwPOf7+d4jIR47/i0r5O/OKRODkJDlV6V1gWIllTwDrjDGdgHWO51UtB/ibMSYcuBi4x/E5eTq2TGCIMaYH0BMYJiIXYycyetUxsVESdqIjT3kAiCryvLrEdqUxpmeRe809/V3mex1YbYzpAvTAfnYejc0Ys8vxWfXEzpp4GvjM03EBiEhr4H6grzHmQuywPWOprL8zY0yt/wEGAmuKPH8SeNLDMYUBO4o83wW0cjxuBeyqBp/b58AfqlNsQACwFRiA7VHpW9p3XMUxhWJPEEOAFYBUh9iAWCC4xDKPf5fYUYb34bhZpTrFViSWq4Hvq0tcFM7d0hQ7NNAK4I+V9XfmFSUCnJskx9NaGGPiHI/jgRaeDEZEwoBewE9Ug9gcVS/bgGPAV8BeINnYCY3As9/pa8BjQJ7jeTOqR2wG+FJEtojIZMcyj3+XQHsgAXjHUZ32togEVpPY8o0FPnI89nhcxpjDwAzgABCHncRrC5X0d+YtiaBGMTa9e+y+XhEJApYADxpjThZd56nYjDG5xhbZQ7HToHap6hhKIyLXAceMMVs8HUspLjXG9MZWid4jIpcXXenBvzNfoDcwxxjTCzhFieoWT/4POOrZRwCflFznqbgc7RIjsUn0PCCQM6uXK8xbEoEzk+R42lERaQXg+H3ME0GIiB82CXxgjPm0OsUGYIxJBtZji8GNHRMagee+00HACBGJxc7LPQRb/+3x2BxXkRhjjmHruvtTPb7LQ8AhY8xPjueLsYmhOsQGNnFuNcYcdTyvDnENBfYZYxKMMdnAp9i/vUr5O/OWRODMJDmeVnSSnvHY+vkqJSKCnSMiyhgzs7rEJiIhItLY8bg+tt0iCpsQxngqLgBjzJPGmFBjTBj27+prY8ytno5NRAJFpEH+Y2yd9w6qwd+ZMSYeOCginR2LrgIiq0NsDuMorBaC6hHXAeBiEQlw/J/mf2aV83fmqcYYDzS2XAvsxtYtP+3hWD7C1vNlY6+OJmLrldcBe4C1QFMPxHUptti7Hdjm+LnW07EB3YFfHHHtAJ51LD8f+BmIxhbj63n4ex0MrKgOsTle/1fHz+/5f/Oe/i6LxNcTiHB8p0uBJtUhNmyVSyLQqMgyj8fliGMasNPxP7AQqFdZf2c6xIRSSnk5b6kaUkopVQZNBEop5eU0ESillJfTRKCUUl5OE4FSSnk5TQRKlSAiuSVGoay0QcZEJEyKjDqrVHXge/ZNlPI66cYOZ6GUV9ASgVJOcozv/7JjjP+fRaSjY3mYiHwtIttFZJ2ItHUsbyEinznmUfhVRC5xHMpHROY5xpb/0tFbWimP0USg1Jnql6gaurnIuhRjzEXAm9hRRwHeAN4zxnQHPgBmOZbPAjYYO49Cb2wPX4BOwGxjTDcgGRjt5vejVLm0Z7FSJYhImjEmqJTlsdgJcmIcg/PFG2Oaichx7Hj12Y7lccaYYBFJAEKNMZlFjhEGfGXsJCeIyOOAnzHmH+5/Z0qVTksESrnGlPHYFZlFHueibXXKwzQRKOWam4v83uR4/AN25FGAW4GNjsfrgLuhYGKdRlUVpFKu0CsRpc5U3zEbWr7Vxpj8W0ibiMh27FX9OMey+7CzbT2KnXnrz47lDwBzRWQi9sr/buyos0pVK9pGoJSTHG0EfY0xxz0di1KVSauGlFLKy2mJQCmlvJyWCJRSystpIlBKKS+niUAppbycJgKllPJymgiUUsrL/T8CdI2XR/3YxwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# output = classifier.predict_generator(test_set, steps=1)\n",
    "# print(test_set.class_indices)\n",
    "# print(output)\n",
    "size = y_test[:,-1]\n",
    "\n",
    "filenames = test_set.filenames\n",
    "nb_samples = len(filenames) / test_batch_size\n",
    "\n",
    "\n",
    "# predict 10 random hand-writing data\n",
    "y_predicted = model.predict(test_set, steps=nb_samples)\n",
    "\n",
    "for x in range(0,size.size):\n",
    "    \n",
    "   np.set_printoptions(suppress=True)\n",
    "   print(\"index:\", x,\n",
    "          \" actual y:\", np.argmax(y_test[x]),\n",
    "          \" answer y:\", np.argmax(y_predicted[x]),\n",
    "          \" prediction:\", np.array(y_predicted[x] * 100))\n",
    "\n",
    "_loss, _acc, _precision, _recall, _f1score = model.evaluate(test_set, steps=nb_samples)\n",
    "print('loss: {:.3f}, accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1score: {:.3f}'.format(_loss, _acc, _precision, _recall, _f1score))\n",
    "# print('loss: ', evaluation[0])\n",
    "# print('accuracy', evaluation[1])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 손실 그래프\n",
    "def plot_loss(history):\n",
    "   # 선 그리기\n",
    "    plt.plot(history.history['loss'], 'y', label='train loss')\n",
    "    plt.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "   # 그래프 제목\n",
    "    plt.title('Model Loss')\n",
    "   # x,y축 이름 표시\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "   # 각 라인 표식 표시\n",
    "    plt.legend(['Train','Validation'],loc=0)\n",
    "\n",
    "# 정확도 그래프\n",
    "def plot_acc(history):\n",
    "  # dir(history.history)\n",
    "    plt.plot(history.history['accuracy'], 'b', label='train accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], 'g', label='val accuracy')\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc=0)\n",
    "\n",
    "plot_loss(history)\n",
    "plt.show()\n",
    "plot_acc(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "idzTf9Bx9FES"
   },
   "outputs": [],
   "source": [
    "# parameter tuning시 사용하는 code\n",
    "\n",
    "\n",
    "# batch_size = [8,16,32,64,128,256]\n",
    "# learning_rate = [0.000001,0.00001,0.0001,0.001,0.01]\n",
    "\n",
    "# # batch_size = [16]\n",
    "# # learning_rate = [0.001]\n",
    "\n",
    "\n",
    "\n",
    "# def recall(y_target, y_pred):\n",
    "#     # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "#     # round : 반올림한다\n",
    "#     y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "#     y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "#     # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "#     count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "#     # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "#     count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "#     # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "#     # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "#     recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "#     # return a single tensor value\n",
    "#     return recall\n",
    "\n",
    "\n",
    "# def precision(y_target, y_pred):\n",
    "#     # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "#     # round : 반올림한다\n",
    "#     y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "#     y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "#     # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "#     count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "#     # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
    "#     count_true_positive_false_positive = K.sum(y_pred_yn)\n",
    "\n",
    "#     # Precision = (True Positive) / (True Positive + False Positive)\n",
    "#     # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "#     precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
    "\n",
    "#     # return a single tensor value\n",
    "#     return precision\n",
    "\n",
    "\n",
    "# def f1score(y_target, y_pred):\n",
    "#     _recall = recall(y_target, y_pred)\n",
    "#     _precision = precision(y_target, y_pred)\n",
    "#     # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "#     _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
    "    \n",
    "#     # return a single tensor value\n",
    "#     return _f1score\n",
    "\n",
    "# for  batch_size, learning_rate in product(batch_size,learning_rate):\n",
    "    \n",
    "#     # Part 2 - Fitting the CNN to the images\n",
    "#     print(f'batchSIZE is {batch_size}, Learning Rate is {learning_rate}')\n",
    "#     print(f'train_epochs is {training_epochs}, validation_step is {validation_steps}')\n",
    "#     # train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "#     #                                    horizontal_flip = True)\n",
    "#     # val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "   \n",
    "#     # test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "#     categories = ['nike_coat_legacy',\n",
    "#               'nike_revolution5',\n",
    "#               'adidas_equipment10'\n",
    "#               ,'converse_chuckTailor_allStarClassic_leather']\n",
    "\n",
    "#     # training_set = train_datagen.flow_from_directory(train_img_dir, target_size=(128,128), \n",
    "#     #                                                  classes=categories, \n",
    "#     #                                                  batch_size=batch_size)\n",
    "\n",
    "#     # test_set = test_datagen.flow_from_directory(test_img_dir,\n",
    "#     #                                             target_size=(128,128), \n",
    "#     #                                             classes=categories, \n",
    "#     #                                             batch_size=batch_size)\n",
    "    \n",
    "#     # val_set = test_datagen.flow_from_directory(val_img_dir,\n",
    "#     #                                     target_size=(128,128), \n",
    "#     #                                     classes=categories, \n",
    "#     #                                     batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "#     # x_train, y_train = next(training_set)\n",
    "#     # x_test, y_test = next(test_set)\n",
    "#     # x_val,y_val = next(val_set)\n",
    "\n",
    "#     def create_model():\n",
    "#       inputs = Input(shape=(299, 299, 3))\n",
    "#       inceptionresnet = InceptionResNetV2(weights = \"imagenet\", include_top=False,input_shape = (299,299,3)\n",
    "#                               ,input_tensor = inputs)\n",
    "#       for layer in inceptionresnet.layers:\n",
    "#           layer.trainable = True\n",
    "          \n",
    "#       output = inceptionresnet.output\n",
    "#       pooling = MaxPooling2D(pool_size=(32,32),padding='SAME')(output)\n",
    "#       flatten1 = Flatten()(pooling)\n",
    "#       dense1 = Dense(units = 1024)(flatten1)\n",
    "#       batch1 = BatchNormalization()(dense1)\n",
    "#       relu1 = ReLU()(batch1)\n",
    "#       dense2 = Dense(units = 512)(relu1)\n",
    "#       batch2 = BatchNormalization()(dense2)\n",
    "#       relu2 = ReLU()(batch2)\n",
    "#       dense3 = Dense(units = 4, activation = 'softmax')(relu2)\n",
    "#       return keras.Model(inputs=inputs, outputs=dense3) \n",
    "      \n",
    "#     model = create_model()\n",
    "\n",
    "#     model.compile(\n",
    "#     optimizer=keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999),\n",
    "#     loss='categorical_crossentropy', \n",
    "#     metrics=['accuracy', precision, recall, f1score],\n",
    "#     )\n",
    "\n",
    "    \n",
    "\n",
    "#     history = model.fit(\n",
    "#     training_set, \n",
    "#     epochs=90,\n",
    "#     validation_data=val_set,\n",
    "#     )\n",
    "\n",
    "\n",
    "#     size = y_test[:,-1]\n",
    "#     print(y_test.size)\n",
    "#     print(size.size)\n",
    "\n",
    "\n",
    "#     # predict 10 random hand-writing data\n",
    "#     y_predicted = model.predict(x_test)\n",
    "#     for x in range(0,size.size):\n",
    "        \n",
    "#       np.set_printoptions(suppress=True)\n",
    "#       print(\"index:\", x,\n",
    "#               \" actual y:\", np.argmax(y_test[x]),\n",
    "#               \" answer y:\", np.argmax(y_predicted[x]),\n",
    "#               \" prediction:\", np.array(y_predicted[x] * 100))\n",
    "\n",
    "#     _loss, _acc, _precision, _recall, _f1score = model.evaluate(x_test, y_test)\n",
    "#     print('loss: {:.3f}, accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1score: {:.3f}'.format(_loss, _acc, _precision, _recall, _f1score))\n",
    "#     # print('loss: ', evaluation[0])\n",
    "#     # print('accuracy', evaluation[1])\n",
    "\n",
    "\n",
    "\n",
    "#     # 손실 그래프\n",
    "#     def plot_loss(history):\n",
    "#       # 선 그리기\n",
    "#         plt.plot(history.history['loss'], 'y', label='train loss')\n",
    "#         plt.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "#       # 그래프 제목\n",
    "#         plt.title('Model Loss')\n",
    "#       # x,y축 이름 표시\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.xlabel('Epoch')\n",
    "#       # 각 라인 표식 표시\n",
    "#         plt.legend(['Train','Validation'],loc=0)\n",
    "\n",
    "#     # 정확도 그래프\n",
    "#     def plot_acc(history):\n",
    "#       # dir(history.history)\n",
    "#         plt.plot(history.history['accuracy'], 'b', label='train accuracy')\n",
    "#         plt.plot(history.history['val_accuracy'], 'g', label='val accuracy')\n",
    "#         plt.title('Model accuracy')\n",
    "#         plt.ylabel('Accuracy')\n",
    "#         plt.xlabel('Epoch')\n",
    "#         plt.legend(['Train', 'Validation'], loc=0)\n",
    "\n",
    "#     plot_loss(history)\n",
    "#     plt.show()\n",
    "#     plot_acc(history)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "tkfC0dEYqFeq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "shoes_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
